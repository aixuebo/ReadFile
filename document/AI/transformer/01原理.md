# 背景与总结
## 1.图
"参考图1"、"参考图2"、"参考图3"、"参考图4"

## 2.为什么有的机器人很聪明（模型很聪明，有的模型很蠢）
就是公式+参数，其中参数很靠谱。他就是聪明。
机器学习就是训练模型的，训练模型的过程就是让参数越来越靠谱的过程。
机器学习最开始的时候，参数都是随机数，所以你可以想象他应该多蠢。所以训练的过程就是让参数越来越靠谱。
参数模型越大，让模型变得越聪明是有帮助的。


# 一、GPT的底层原理
## 1.case：蜜蜂是如何酿造蜂蜜的
GPT真的懂问题和回答吗，其实还真未必懂。
* GPT会提取[核心]关键词,蜜蜂和蜂蜜，并根据自己的知识，找到与这些词相关的其他词汇，比如花粉、蜂巢。
* 按照正确的语法和逻辑关系，将这些词语组织称一个完整的回答。

## 2.GPT = generative pre-trained transformer 生成式预训练转换器。
重点是transformer。

## 3.论文
《Attention is all you need》 注意力是你唯一需要的东西。2017年发表。
transformer的核心函数叫做Attention注意力

# 二、transformer论文 -- 模型结构 -- 非常简单的结构

## 1.encoding和decoding 编码器与解码器
模型 = 公式 + 参数系数。
* 比如 f(x) = ax + by
输入的内容，就是x和y等内容。
* GPT3.5的参数：1750亿个参数。人类的神经元也是几千亿个，他俩是一个量级的。

## 2.encoding 编码器
做一个map映射，将一个输入x序列 --> 映射成z序列。
比如中秋节你应该吃什么。
[x1,x2..],每一个序列位置就是一个token。(你可以简单理解成每一个汉字)
[z1,z2...],z序列是一个向量序列，即相当于是一个二维矩阵，比如z1表示512维度的内容，表示描述了x序列中，每一个token的内容。
（为什么是512呢，有可能是他尝试了很多实验，发现512是效果最好的参数；512是一个可调参数，不是一个固定的）

## 3.decoding 解码器
* 输出是y序列，每次仅输出一个元素；
* 每次输出是一个自回归的。

主要是为了简化模型复杂度，每一次瞬时你需要给我崩一个字就可以了。
因为每一个输出只有一个汉字，而中文常用汉字就只有3000个，你可以暴力遍历尝试，将每一个迭代的汉字与前面的输出结果联起来，看看连贯性以及摆在这里合理的概率，这个叫做子回归。

比如：
* z序列喂给decoding，他会输出y1。
* z序列+y1再喂给decoding，他会输出y2。
* z序列+y1+y2再喂给decoding，他会输出y3。

## 4.架构图就可以读懂了
* 输入 + embedding，形成计算机认识的语言。即每一个输入的token转换成512个向量。
* 加入位置信息，可以实现并行计算。
比如：猫咬了狗；狗咬了猫。文字一样，但含义不一样，所以要把位置信息放进输入里去。
* N*,说明可以迭代N轮循环嵌套。
* 注意力模型
* Add & norm 表示结果的运算以及归一化操作。
* 输出z序列。
* 解码器有两个输入：编码器的输出z序列 + output已经输出的内容。

## 5.优势1 -- 并行计算
非常适合GPU的并行计算。在设计整个架构的时候，都在思考如何让自己的算法必须跑到GPU上参与并行计算。
与RNN(循环神经网络)相比，RNN是不支持并行计算的。
① RNN的做法:
* 每一个token转换成512个向量。
* y1 --> 输出 z1. 即包含"中"的信息。
* y2 + z1 --> 输出 z2. 即包含"中秋"的信息。
因此他类似递归，输入问题就开始串行递归计算，下一步计算前依赖上一层的结果，所以复杂度高，效率很慢。

② transformer的做法 -- 参考图1
x序列转换成z序列的时候，有两步
* 每一个token转换成511个向量。即组成a序列。
* 每一个token的序号，组成b序列。比如b[i] = (a[i],序号)
* b序列转换成z序列。

比如"中秋节吃什么"，b4只跟"吃"+4位置有关系，与"吃"前后的内容语义无关系。因此是可以参与并行计算的，彼此之间都是解耦独立的。

## 6.优势2 -- 注意力机制和自注意力机制


# 三、注意力机制和自注意力机制
## 1.注意力机制 -- 参考图2 -- 算法的潜台词是:我们要把注意力放在更相关性的样例上
* 更像是查询系统，输入身高，查询对应的体重。
比如有一个身高和体重的映射表，当给定一个身高时，会计算给定的身高与数据库所有身高做相似度计算，计算好一个权重(比如相减开根号等)，权重归一化。
因此体重 = sum(权重 * 体重)。

* 当然输入可以多维的，因为数据库也是多维的。甚至输出也可以是多维的。
通过 身高+胸围+腿长 --> 预测 身高+腰围

## 2.K、V、Q
* K:与输入维度相同，用于与维度计算权重Q，比如身高+胸围+腿长
* V:用于计算输出的内容。比如 身高+腰围
* Q:计算好的权重

K和Q维度要相同。
V可以不与K维度相同。

## 3.泛化推理 -- 512维的K和Q
即用512维度描述通用物品。
给定任何一个输入，转换成512维度，与已有的512维度K进行相似性计算。

* 此时K就是一个问题。v就是一个答案。输入就是一个新问题。
* 即计算新问题与已有问题的相关性，用相关性作为权重，与v进行加权求和，得到输出向量。
你的问题与已存在的问题越像，输出的结果也就是权重最高的答案的结果的融合版本。得到匹配你预期的答案的概率也就越高。
* 输出向量decoding转换成人类可识别的文字。

## 4.自注意力机制 -- 参考图3 -- 算法的潜台词是:问题K、答案V、权重Q都完全一样的时候，识别问题中token之间的重要程度，即有一些token其实删除了也无所谓。
① 自注意力机制更像是无监督学习。
重点，比较难，也更有泛化能力，因此transformer统一了模态，支持语言模型、视频模型、视觉模型、语音模型。就是因为他有泛化能力。

② 架构图中，下面两个注意力都是自注意力机制。
只有右侧，解码器，上部分的是 注意力机制。

③ 计算逻辑
比如 问"中秋节吃什么"
* 因为Q K V内容一样。此时问题内容就是Q K V任何一个都行。
* "中"与K中 每一个汉字去计算相关度系数。
* 相关度系数 * V，得到输出序列[c1,c2..] 

④ 如果是一段话，而不是一句话呢
上下文去寻找一个窗口宽度，即比如支持1024个token，则每次计算相关的，都是用当前上下文所有的token内容，彼此计算相关性，然后计算自注意力机制，得到序列c，这个是在做什么呢？
比如你问我答，你问我答，可能都回答了十几轮；
每次都拿到这十几轮的数据，去做自注意力机制，得到序列c，这个是在做什么呢？

⑤ 输出的c序列是什么呢？
类似无监督学习，帮助我们提取一段话里面，字和词之间的相关性、重要性、逻辑关联性。
比如“结构巨简单，但完全不知道在干嘛”，你把巨、但完全，去掉，无所谓；你把“不”去掉，就肯定不行了。
因此结论是有一些字跟每一个字的相关度都很低，那就表示不重要。

比如 输出：中秋节最具有代表性的食物是月饼，他的外皮酥脆，包含很多香料。
* 当输出"包"的时候，也要做自助力机制，可能大概率出现"含"。
* 他，指代什么，也是可以表示出来的。代表月饼、中秋节、搞点？他是可以搞定的。

⑥ 自注意力机制就是在模拟人脑
因为人脑在听到一句话的时候，会非常不自觉的去提取重要的词和字，然后去处理重要的字和词。
比如:"左边是一个编码器模型，右边是一个解码器模"，你会非常不自觉的关注和处理重要的词语"编码器、解码器"，忽略 "是一个"。

⑦ 无监督学习，避免了标注
解放算法工程师的工作内容:以下的内容其实是无效的工作。
因此算法工程师越来越不重要了。不需要算法工程师做额外的处理。

* 自然语言中，标注动词、名词。
* 分词标注。
* 多模态，视觉处理
大模型处理视觉数据很奇怪，因为大模型全称是大语言模型（LLM large langue model）。凭什么能处理视觉数据呢。
过去NLP和视觉算法，论文、工程师，都是两拨人，没有多少交集的。
而自注意机制，放在视觉上居然是通用的。
比如你看一下你左边，你的眼睛一定会捕获到视觉数据，传输到你的大脑，你肯定关注的重要程度不同，比如你肯定没有关注到右边窗户右下角我写的一排小字。
即眼睛传输到大脑也会提取重要内容，忽略不重要内容。
* 视觉处理
边缘提取、特征提取，帮机器做助理工作，帮机器描绘物体边缘，把边缘描绘成黑色的。
人看到一个图，肯定不需要先描边，对吧。


# 四、应用技术
## 1.第一阶技术 -- 最简单的 --- 指令工程 prompt

## 2.第二阶技术 -- 向量数据库、向量检索、LangChain、SK(semantic-kernel)、Agent智能体(aotu gpt)。

## 3.第三阶技术 -- fine-tune --- 专业领域的问答(旅游线路规划、医疗问诊、教育培训、助教答疑、律师答疑)，需要自己定制这个领域的垂直大模型
模型定制/模型微调，就是给模型的参数微微调节的机会。

## 4.fine-tune的三个步骤 -- 参考图4
* ① 收集数据，并制定监督测量。 --- 此时得到一个新的微调后的模型。
组织很多人，去写很多问题和答案。
如果没有人力，可以准备很多prompt，调GPT接口，得到答案。

* ② 生成式AI
收集比较数据，并且训练奖励模型 — fine-tuning
基于微调后的模型，去问问题，让他回答4个结果。
然后找人对问题的答案做排序。
基于这些排序，训练奖励模型。类似于积分系统，人类满意的答案会有更高的积分奖励。

* ③ 使用强化学习，针对奖励模型优化策略
基于2的模型，在做一轮微调。得到更新的模型。

* ④ 调教模型 -- 循环2-3步骤，循环N论，不断地就有更好的模型。

## 5.优化方式
① 调教要尽可能的先提早进行
让运营和服务人员先进行尝试，比如客服接到消息后，大模型针对问题，生成多套回复内容，如果客服针对某一个内容还不错，点复制按钮，此时相当于人类给了一个反馈，该问题获得奖励。获取埋点信息即可用于下一轮的训练了。
② 为什么midjour画图软件，会返回4个图？并且为什么会清晰度不够的图
也是因为让用户去反馈；
如果都觉得不好，则重新生成。-- 人类反馈生成的不好。
当你选择一个图后，获取分辨率更高的图。--- 人类反馈喜欢。

## 6.fine-tune需要的数据量 --- 非常低的数据量
① 需要1.3万条QA，就够了。
律师咨询，准备两个运营，是不是2-3周就能获取到了。从企业微信里拿历史问题，是不是就够了。
② 33000条用户排序的结果，就够做一轮训练了。

## 7.案例：如何自己做fine-tune
① alpaca(羊驼的一个github模型)由meta的LLaMa 7B微调而来的模型。
用了52000条原始QA数据，性能效果与GPT3.5差别不大。

关键是总花费不到600美元。
* 因为是个人，所以没有QA那么多资源，就拿问题去掉接口，获取答案，花费500美元。
* 租用8个80G的A100显卡，训练3小时，不到100美元。

② 细节
LLaMa，是facabook开源的一个模型。
7B 表示LLaMa的一个版本，表示70亿个参数。(其实是很小的参数)
13B，表示130亿个参数。

③ 私有化部署一个7B的模型成本
单张4090的显卡就够了，成本很低。跟租一个服务器差不多成本。

