一、第一阶段
以快速迭代需求为导向，底层实时计算处理大部分用Storm流式清洗、过滤后，数据存储到Mysql、Eagle、Cache等，上层在存储之上做数据指标计算
各业务线数据缺少共性沉淀，按需进行烟囱式开发。
大部分是case by case 从底向上的开发，维护成本和扩展成本较高

二、第二阶段
为了解决实时计算1.0架构的问题，在内部建立了底层明细的数据模型，在数据上有了一定的复用性。分层建设了ods和dwd层数据。
明细层数据落地分布式Mysql、Eagle、Doris等存储引擎，上层应用以RBC批量查询进行数据汇总计算，对外输出汇总数据。
沉淀了部分业务线共性DWD数据，缺少汇总层数据计算

三、第三阶段
基于第二阶段数据建设了DWD明细层，但是缺少DWS(topic)层数据。主要建设DWS、ADS层数据(app层宽表+维表)

四、遇到的问题以及解决方案

问题1：订单状态变化过程，获取最新值
1.订单履约过程发生状态流转时，由于上游业务系统只有更新时间，后续数据统计需求会根据各个订单的状态时间做很多业务需求分析和计算，需要将订单各个状态的时间带到下游，并且以最新的数据进行后续的数据处理。

2.解决思路：
数据状态时间打平：扩展业务时间字段。将一个时间+状态，转换成多个状态+每一个状态的时间。
binlog解析到每一次变更日志，提取每一个状态 以及 对应的时间点。因此一个状态会对应多个时间点。我们根据状态获取最新的时间点即可表示最新的状态时间数据。

从原来的state+时间两个字段，扩展成：状态1 时间、状态2 时间、状态3 时间等。

3.实现方案:
spark解析日志,落盘到doris,key是状态,value是max(时间)。


问题2:存储层技术选型
实时计算,主要涉及数据清洗、存储、合流成宽表到存储层。  以及 定时触发任务完成指标计算，存储到存储层。

1.面对的问题:
a.开发效率低,涉及宽表,存储,计算一套下来，一个需求需要开发3d左右。
b.维度爆炸的挑战,需要每一个维度组合计算好(cube),然后将结果存储,为服务层提供查询服务。避免服务层现算,导致查询慢。

2.解决思路
a.使用olap引擎,支持现算。避免维度组合爆炸，提高开发速度。把计算问题丢给引擎。
b.维表多,并且容易变化,比如组织结构,因此不能成宽表(宽表会有维度变化后,回刷数据的问题),因此引擎需要支持join。
c.olap引擎需要有预聚合能力，方便建设aggr层数据。
d.查询效率 以及 处理复杂查询(子查询)等方面有不错的效果。

3.最终选择doris,支持上面4个能力。


五、如何使用doris落地实时数仓建设
1.v1.0
a.批处理,底层明细数据kafka2doris实时摄入到Doris，然后Doris2Doris的sql计算逻辑,每15min进行一次调度汇总，批量更新指标数据。
b.整个加工过程,对数据分层，每一层都是15分钟进行调度。明细层（存储明细数据），汇总层（存储粗粒度的汇总数据），应用层（按照规则汇总、计算出业务指标）
c.准实时方案优势：
kafka2doris，秒级延迟。
通过ETL定时作业，批量汇总明细数据到聚合数据，优化查询效率。
通过SQL进行汇总逻辑开发，开发效率较高。
数据最终落回到Doris，快速对接下游的BI分析工具和系统。

2.v2.0
1.准实时的运力看板运行一段时间之后，我们发现准实时方案存在以下几个问题：
汇总任务Doris2Doris执行时间变长：定时批量汇总明细数据，随着数据量增大，汇总时间越来越长。
数据展示延迟：数据定时汇总，会有一定的延迟时间，并没有做到实时。
Doris资源占用大：定时任务批量更新计算数据，占用大量的BE资源，影响查询性能。---即doris面向用户查询要求响应快,但有一部分资源用于计算数据,能否将这部分不合理的资源干掉,提升查询效率。

2.针对以上问题，我们必须转变思想，找到新的解决方案：批处理转流处理，全量更新转增量更新。
上面问题主要集中在数据每次查询都是全量的,即使有分区，也是查询计算的数据较大，因此需要转变思想，用增量的方案解决。

3.解决思路：
kafka ---> flink(ods、明细、15分钟内aggr聚合) --> 存储doris(千万级别数据量) --> 对外提供查询
通过FlinkSQL编写汇总与计算逻辑，数据在流中处理，增量代替批量，减少计算耗时，减少Doris的资源占用。
流计算后直接入库Doris，Doris只作为存储引擎使用，实现秒级实时查询效果。


4.实时方案优势：
流式处理，不受数据量增多影响。
增量计算，时效性高，做到秒级延迟。
FlinkSQL编写计算逻辑，开发效率较高。
汇总结果指标实时同步到Doris，Doris只作为存储，资源占用少。

