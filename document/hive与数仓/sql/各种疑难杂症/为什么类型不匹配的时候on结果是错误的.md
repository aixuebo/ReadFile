# 背景与总结
A表的关联字段是bigint，B表的关联字段是string，为什么结果是错误的？会丢失一些明明可以匹配的数据

答案是Spark Sql Doule精度丢失导致的。

# 一、原因分析
* 分析任务执行计划的时候，会发现，左表和右表，都进行了类型转换，都转换成了double。
与我们预期的转换成bigint，或者string 有出处，原因下面会解释。

* 如果string超过18位，转换成double就会丢弃。
验证id = 1582657571288814597转化为double的结果.
结果是1.58265757128881459E18 (意味着有效位是18位，小数点后面是17位，前面有1位)


## 上述结果，确实佐证了，
第一、wiki里面对于双浮点精度的说法是准确的，有17个有效小数位精度。
第二、长数值(大于18位的)转成double类型后，忽略了后面的数字(这里忽略了最后一位)。

## 修复
cast(t2.app_order_id as bigint)即可。
底层原理是 它会将app_order_id转化成为decimal(20,0).能够覆盖19位,完结。

# 二、为什么系统底层要转换成double呢
## 1.兼容性的考虑
case
如果左边是整数123右边是字符串123.00。这个时候都按照字符串处理。结果是不相等的。但是如果按照double处理是相等的。

## 2.因为更好的兼容性 -- 深层考虑。
* 当发现左右两边有一个数字时，明确了是数字比较，所以转换成double和bigint更好，选择double又更有兼容性，所以不选择string。
* 从比较上看，string和double也不一样
“4.12” 和 123 谁大
字符串认为4比1大，所以比较结束了。
而数字比较，是比较全部，4.12 和 123 谁大，结论是123大.
* join的条件也有可能是不等的，比如大于等于，小于之类的。
大家往往认为on仅用于=比较。


这个思考框架应该不是要考虑举个这么细，是否一定是等于，有没有可能是不等的。
这不是做系统底层的思考方式，底层思考方式是严谨 + 兼容性，因为上面怎么用，那都随机的
未来的不确定性 是底层向上兼容的潜规则。

# 三、现实遇到的问题
流量表时，有一些id会非常大，超过了18位，所以造成的问题在流量表会更常见。