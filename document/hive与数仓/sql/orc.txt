详细参见pdf项目中orc的原始论文---里面有详细的内部实现图

一、创建orc文件
1.三种方式
a.CREATE TABLE ... STORED AS ORC
b.ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC
c.SET hive.default.fileformat=Orc
2.默认属性
属性name、  默认属性值 、备注
orc.bloom.filter.columns	""	comma separated list of column names for which bloom filter should be created
orc.bloom.filter.fpp	0.05	false positive probability for bloom filter (must >0.0 and <1.0)
orc.compress    ZLIB     high level compression (one of NONE, ZLIB, SNAPPY)
orc.compress.size   262144   number of bytes in each compression chunk
orc.create.index    true    whether to create row indexes
orc.row.index.stride    10,000  number of rows between index entries (must be >= 1000)
orc.stripe.size 67,108,864  number of bytes in each stripe

3.创建语法以及如何配置属性
create table Addresses (
  name string,
  street string,
  city string,
  state string,
  zip int
) stored as orc tblproperties ("orc.compress"="NONE");

注意tblproperties ("orc.compress"="NONE") 可以删除,不需要一定有

4.创建表后的描述信息
a.show create table 显示额外的信息
ROW FORMAT SERDE
  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'

b.desc formatted 显示的额外信息
SerDe Library:          org.apache.hadoop.hive.ql.io.orc.OrcSerde
InputFormat:            org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
OutputFormat:           org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
Compressed:             No

二、orc文件的维护
1.如果查看文件统计内容  hive --orcfiledump path | more
注意:path为目录,不能支持*方式
2.其他方式
hive --orcfiledump <location-of-orc-file>

// Hive version 1.1.0 and later:
hive --orcfiledump [-d] [--rowindex <col_ids>] <location-of-orc-file>

// Hive version 1.2.0 and later:
hive --orcfiledump [-d] [-t] [--rowindex <col_ids>] <location-of-orc-file>

// Hive version 1.3.0 and later:
hive --orcfiledump [-j] [-p] [-d] [-t] [--rowindex <col_ids>] [--recover] [--skip-dump]
    [--backup-path <new-path>] <location-of-orc-file-or-directory>

Specifying -d in the command will cause it to dump the ORC file data rather than the metadata (Hive 1.1.0 and later).
Specifying --rowindex with a comma separated list of column ids will cause it to print row indexes for the specified columns, where 0 is the top level struct containing all of the columns and 1 is the first column id (Hive 1.1.0 and later).
Specifying -t in the command will print the timezone id of the writer.
Specifying -j in the command will print the ORC file metadata in JSON format. To pretty print the JSON metadata, add -p to the command.
Specifying --recover in the command will recover a corrupted ORC file generated by Hive streaming.
Specifying --skip-dump along with --recover will perform recovery without dumping metadata.
Specifying --backup-path with a new-path will let the recovery tool move corrupted files to the specified backup path (default: /tmp).
<location-of-orc-file> is the URI of the ORC file.
<location-of-orc-file-or-directory> is the URI of the ORC file or directory. From Hive 1.3.0 onward, this URI can be a directory containing ORC files.

命令执行后查看信息,了解ORC的内部结构
1.展示加载的路径以及hive版本
2.展示该文件的汇总信息--比如{include: null, offset: 0, length: 9223372036854775807}即总长度
3.展示该文件包含的schema结构
a.有多少个子节点,通过序号展示
b.每一个属性的name
c.每一个数据类型
4.该文件的保存了多少行数据
5.该文件存储的压缩格式以及压缩后的大小
6.Stripe Statistics ,stripe维度的统计信息
 循环每一个Stripe,展示该Stripe下每一个列对应的非null的行数以及是否有null值,最小值 最大值 sum等统计信息
7.File Statistics 文件维度的统计信息
  记录该文件下每一个列对应的非null的行数以及是否有null值,最小值 最大值 sum等统计信息
8.展示每一个Stripes的信息
a.该Stripe在文件中的偏移量 开始位置、数据字节、index字节、footer字节、行数信息:offset: 3 data: 34780967 rows: 1809559 tail: 188 index: 33984
b.记录每一个列对应的index信息,展示该index的偏移量--开始偏移位置和该列的index所占用字节长度
c.记录每一个列具体的data信息,展示该data的偏移量--开始偏移位置和该列的data所占用字节长度
  并且如果是String还要存储是否是null的数据、数据内容、数据长度、字典内容
d.footer,记录每一个列的编码信息
9.结尾记录统计信息
File length: 34815715 bytes
Padding length: 0 bytes
Padding ratio: 0%

全文内容:
hive --orcfiledump path/000001_0.c1
Processing data file path/000001_0.c1 [length: 34815715]
Structure for path/000001_0.c1
File Version: 0.12 with HIVE_8732
17/06/20 13:42:20 INFO orc.ReaderImpl: Reading ORC rows from path/000001_0.c1 with {include: null, offset: 0, length: 9223372036854775807}
17/06/20 13:42:20 INFO orc.RecordReaderImpl: Schema on read not provided -- using file schema [kind: STRUCT
subtypes: 1
subtypes: 2
subtypes: 3
subtypes: 4
subtypes: 5
subtypes: 6
subtypes: 7
subtypes: 8
subtypes: 9
fieldNames: "_col0"
fieldNames: "_col1"
fieldNames: "_col2"
fieldNames: "_col3"
fieldNames: "_col4"
fieldNames: "_col5"
fieldNames: "_col6"
fieldNames: "_col7"
fieldNames: "_col8"
, kind: STRING
, kind: STRING
, kind: DOUBLE
, kind: DOUBLE
, kind: DOUBLE
, kind: DOUBLE
, kind: DOUBLE
, kind: DOUBLE
, kind: DOUBLE
]
Rows: 1809559
Compression: ZLIB
Compression size: 262144
Type: struct<_col0:string,_col1:string,_col2:double,_col3:double,_col4:double,_col5:double,_col6:double,_col7:double,_col8:double>

Stripe Statistics:
  Stripe 1:
    Column 0: count: 1809559 hasNull: false
    Column 1: count: 1809559 hasNull: false min: 0000017b7a86447a866631e3dc06dcc9 max: fffff732a65c4369b4ca8c84baaae760 sum: 57905888
    Column 2: count: 68046 hasNull: true min: 2017-05-14 max: 2017-06-19 sum: 680460
    Column 3: count: 1809559 hasNull: false min: 0.0 max: 9034960.96 sum: 2.280400287570001E9
    Column 4: count: 1809559 hasNull: false min: 0.0 max: 9034420.46 sum: 2.277510089340001E9
    Column 5: count: 1809559 hasNull: false min: 0.0 max: 3845000.0 sum: 6.193470014E8
    Column 6: count: 1809559 hasNull: false min: 0.0 max: 3848000.0 sum: 6.138810014E8
    Column 7: count: 1809559 hasNull: false min: 0.0 max: 769820.0 sum: 4.5581980169999965E7
    Column 8: count: 1809559 hasNull: false min: 0.0 max: 200000.0 sum: 4.309911587000001E7
    Column 9: count: 1809559 hasNull: false min: 0.0 max: 370000.0 sum: 8384000.0

File Statistics:
  Column 0: count: 1809559 hasNull: false
  Column 1: count: 1809559 hasNull: false min: 0000017b7a86447a866631e3dc06dcc9 max: fffff732a65c4369b4ca8c84baaae760 sum: 57905888
  Column 2: count: 68046 hasNull: true min: 2017-05-14 max: 2017-06-19 sum: 680460
  Column 3: count: 1809559 hasNull: false min: 0.0 max: 9034960.96 sum: 2.280400287570001E9
  Column 4: count: 1809559 hasNull: false min: 0.0 max: 9034420.46 sum: 2.277510089340001E9
  Column 5: count: 1809559 hasNull: false min: 0.0 max: 3845000.0 sum: 6.193470014E8
  Column 6: count: 1809559 hasNull: false min: 0.0 max: 3848000.0 sum: 6.138810014E8
  Column 7: count: 1809559 hasNull: false min: 0.0 max: 769820.0 sum: 4.5581980169999965E7
  Column 8: count: 1809559 hasNull: false min: 0.0 max: 200000.0 sum: 4.309911587000001E7
  Column 9: count: 1809559 hasNull: false min: 0.0 max: 370000.0 sum: 8384000.0

Stripes:
  Stripe: offset: 3 data: 34780967 rows: 1809559 tail: 188 index: 33984
    Stream: column 0 section ROW_INDEX start: 3 length 37
    Stream: column 1 section ROW_INDEX start: 40 length 10738
    Stream: column 2 section ROW_INDEX start: 10778 length 2732
    Stream: column 3 section ROW_INDEX start: 13510 length 3741
    Stream: column 4 section ROW_INDEX start: 17251 length 3739
    Stream: column 5 section ROW_INDEX start: 20990 length 2568
    Stream: column 6 section ROW_INDEX start: 23558 length 2580
    Stream: column 7 section ROW_INDEX start: 26138 length 2847
    Stream: column 8 section ROW_INDEX start: 28985 length 2989
    Stream: column 9 section ROW_INDEX start: 31974 length 2013
    Stream: column 1 section DATA start: 33987 length 32444694
    Stream: column 1 section LENGTH start: 32478681 length 15842
    Stream: column 2 section PRESENT start: 32494523 length 86570
    Stream: column 2 section DATA start: 32581093 length 43491
    Stream: column 2 section LENGTH start: 32624584 length 7
    Stream: column 2 section DICTIONARY_DATA start: 32624591 length 98
    Stream: column 3 section DATA start: 32624689 length 791308
    Stream: column 4 section DATA start: 33415997 length 788100
    Stream: column 5 section DATA start: 34204097 length 181963
    Stream: column 6 section DATA start: 34386060 length 181025
    Stream: column 7 section DATA start: 34567085 length 87732
    Stream: column 8 section DATA start: 34654817 length 90558
    Stream: column 9 section DATA start: 34745375 length 69579
    Encoding column 0: DIRECT
    Encoding column 1: DIRECT_V2
    Encoding column 2: DICTIONARY_V2[37]
    Encoding column 3: DIRECT
    Encoding column 4: DIRECT
    Encoding column 5: DIRECT
    Encoding column 6: DIRECT
    Encoding column 7: DIRECT
    Encoding column 8: DIRECT
    Encoding column 9: DIRECT

File length: 34815715 bytes
Padding length: 0 bytes
Padding ratio: 0%


三、orc文件的介绍
1.与RCfile比较后的有点
a.每一个task输出一个单独的文件,减少了NameNode的压力
b.支持hive的数据类型,比如datetime, decimal, and the complex types (struct, list, map, and union)
c.轻量级的索引存储在文件中
skip row groups that don't pass predicate filtering
seek to a given row
d.block-mode 压缩基于数据类型的
run-length encoding for integer columns
dictionary encoding for string columns
e.使用单独的RecordReaders对象对相同的文件可以并发读取
f.无需扫描markers就可以分割文件
g.bound the amount of memory needed for reading or writing 读和写的时候需要绑定内存,防止内存溢出
h.元数据存储使用Protocol Buffers, 因此允许添加和移除field属性

四、如何存储整数和String
1.存储整数2个流
present bit stream: is the value non-null? 该present流存储value是否是null
data stream: a stream of integers,数据流存储真正的int数据
2.存储String,需要4个流
present bit stream: is the value non-null?
dictionary data: the bytes for the strings
dictionary length: the length of each entry
row data: the row values

五、存储文件架构
1.文件架构
a.orc文件包含多个由row数据组成的group组,称之为stripes,即一个stripes代表若干行数据的集合。
格式为 index data、row data、stripe footer
其中index data和row data都是以列为存储的,即每一列都有一个index和data组成
b.并且伴随着附属信息存储在文件的footer里面。
该部分包含文件中的stripes集合,
以及每一个stripe里面有多少行数据,
以及每一个列的数据类型。
还有每一个列的聚合值,比如count, min, max, and sum.
c.文件的最后有一个postscript,里面记录这压缩的参数以及footer的大小
d.默认stripe的size为250M,该数据,说明从HDFS上读取数据的效率越高。

2.详细的每一个Stripe的架构
a.每一个stripe由三部分组成,即index data, row data, and a stripe footer.
b.footer包含了一个字典流的地址locations
c.row data被适用于table的扫描
d.index data包含每一列的最小值和最大值以及每一列的行的位置,以及A bit field or bloom filter could also be included.
e.row index实体提供了offsets,可能seek到正确的压缩快




原理
1.一般存储方式的优缺点
a.水平的行存储结构： 行存储模式就是把一整行存在一起，包含所有的列，这是最常见的模式。
每一次要全部查询所有的列,大量无用的列被加载了.同时不同列数据不一样,压缩比也不太理想

b.垂直的列存储结构： 列存储是将每列单独存储或者将某几个列作为列组存在一起。
避免了水平方式的两个缺点
但是产生了新的缺点,即重建的时候比较差.尤其一行数据不同列在不同的数据块时候,比如从第1块获取第一个列,从第2块获取第二列,然后又合并的过程很费劲,需要比较大的网络开销和运算开销。


RCFile 的设计和实现
1.数据的布局：
首先根据 HDFS 的结构，一个表可以由多个 HDFS 块构成。在每个 HDFS 块中，RCFile 以 row group 为基本单位组织数据，一个表所有 row group 大小一致，一个 HDFS 块中可以包含多个 row group。
每个 row group 包含三个部分，
第一部分是 sync marker，用来区分一个 HDFS 块中两个连续多 row group。
第二部分是 row group 的 metadata header，记录每个 row group 中有多少行数据，每个列数据有多少字节，以及每列一行数据的字节数（有多少行就有多少个数字,用于定位数据）。
第三部分就是 row group 中的实际数据，这里是按列存储的。

2.数据的压缩：
在 metadata header 部分用 RLE (Run Length Encoding) 方法压缩，因为对于记录每个列数据中一行数据的字节数是一样的，这些数字重复连续出现，因此用这种方法压缩比比较高。在压缩实际数据时，每列单独压缩。
即因为可能一个列中的数据相同的内容会很多,比如男女等,因此会提高很大的压缩比

3.数据的追加写：
RCFile 会在内存里维护每个列的数据，叫 column holder，当一条记录加入时，首先会被打散成多个列，人后追加到每列对应的column holder，同时更新 metadata header 部分。
可以通过记录数或者缓冲区大小控制内存中 column holder 的大小。当记录数或缓冲大小超过限制，就会先压缩 metadata header，再压缩 column holder，然后写到 HDFS。

4.数据的读取和惰性解压（lazy decompression）：
RCFile 在处理一个 row group 时，只会读取 metadata header 和需要的列，合理利用列存储在 I/O 方面的优势。
而且即使在查询中出现的列技术读进内存也不一定会被解压缩，只有但确定该列数据需要用时才会去解压，也就是惰性解压（lazy decompression）。
例如对于 select a from tableA where b = 1，会先解压 b 列，如果对于整个 row group 中的 b 列，值都不为 1，那么就没必要对这个 row group 对 a 列去解压，因为整个 row group 都跳过了。

