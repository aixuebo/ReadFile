目标1000万日单


方案
一、分库分表方案
1.分16个库，每一个库分16个表。
每天每个库存储62.5w条数据（1000万/16）。
2.每秒最高订单量理论上在1250左右（ 2000*（62.5/100） ）
3.两年，一共总订单数73亿(73亿=1000万*365*2).
每一个库存储4.56亿（62.5w*365*2）
4.每个表存储 2850万（2850万=4.56亿/16），mysql也够存。
拆分表的数量是2的N次幂,原因是可以位运算加速路由。比取模运算效率高

二、分区方案
1.按照user_id做为sharding。
分库分表主要是为了用户端下单和查询使用。
将相同用户的订单数据存储到同一个数据库的同一张表中。
查询订单时只需要路由到一张表就可以获取用户的所有订单了。
按user_id的查询频率最高，其次是order_id。

2.订单id 由 用户id+订单id组成。 
用户查询订单id时，先解析拆分成user，路由找到对应的库+表。
然后在单库里查询订单id,是非常快的。

三、设计优化
1.上层做redis,存储活跃用户最近50条订单。这样只有Redis查不到订单的用户请求才会到数据库查询订单，这样就减小了数据库查询压力。
2.每个分库还有两个从库，查询操作只走从库，进一步分摊了每个分库的压力。


四、查询不是以user或者orderid查询时,如何解决。
不同用户的订单数据散落在不同的库和表中，如果需要根据用户ID之外的其他条件查询订单。例如，运营同学想从后台查出某天iphone7的订单量
1.hive离线查询 或者 实时流查询。
2.ES+hbase
参与检索的数据存放到ES中,比如商家，商品名称，订单日期。value是订单id。
所有订单数据全量保存到HBase中

我们知道HBase支持海量存储，而且根据rowkey查询速度超快。而ES的多条件检索能力非常强大。可以说，这个方案把ES和HBase的优点发挥地淋漓尽致。
看一下该方案的查询过程：先根据输入条件去ES相应的索引上查询符合条件的rowkey值，然后用rowkey值去HBase查询，后面这一步查询速度极快，查询时间几乎可以忽略不计。

3.Mysql中的订单数据需要实时同步到Hbase和ES中。同步方案是什么？
我们利用Canal实时获取Mysql库表中的增量订单数据，然后把订单数据推到消息队列RocketMQ中，消费端获取消息后把数据写到Hbase，并在ES更新索引。

4.每天产生数百万的订单数据，如果管理后台想查到最新的订单数据，就需要频繁更新ES索引。在海量订单数据的场景下，索引频繁更新会不会对ES产生太大压力？
ES索引有一个segment（片段）的概念。ES把每个索引分成若干个较小的 segment 片段。
每一个 segement 都是一个完整的倒排索引，在搜索查询时会依次扫描相关索引的所有 segment。
每次 refresh（刷新索引） 的时候，都会生成一个新的 segement，
因此 segment 实际上记录了索引的一组变化值。由于每次索引刷新只涉及个别segement片段，更新索引的成本就很低了。
所以，即便默认的索引刷新（refresh）间隔只有1秒钟，ES也能从容应对。
不过，由于每个 segement 的存储和扫描都需要占用一定的内存和CPU等资源，
因此ES后台进程需要不断的进行segement合并来减少 segement 的数量，从而提升扫描效率以及降低资源消耗。

五、运维 -- 不停机数据迁移
1.数据迁移过程我们要注意哪些关键点呢？
第一，保证迁移后数据准确不丢失，即每条记录准确而且不丢失记录；
第二，不影响用户体验，尤其是访问量高的C端业务需要不停机平滑迁移；
第三，保证迁移后的系统性能和稳定性。


2.常用的数据迁移方案主要包括：挂从库，双写以及利用数据同步工具三种方案。下面分别做一下介绍。

3.挂从库
这种方式适合表结构不变，而且空闲时间段流量很低，允许停机迁移的场景。一般发生在平台迁移的场景，如从机房迁移到云平台，从一个云平台迁移到另一个云平台。大部分中小型互联网系统，空闲时段访问量很低。在空闲时段，几分钟的停机时间，
对用户影响很小，业务方是可以接受的。所以我们可以采用停机迁移的方案。步骤如下：

新建从库（新数据库），数据开始从主库向从库同步。
数据同步完成后，找一个空闲时间段。为了保证主从数据库数据一致，需要先停掉服务，然后再把从库升级为主库。(更改请求ip)
最后启动服务，整个迁移过程完成。

4.双写
老库和新库同时写入，然后将老数据批量迁移到新库，最后流量切换到新库并关闭老库读写。
步骤如下：
a.代码双写。
b.新库同步老库数据，并且做好新老库数据一致性校验脚本。
c.开启双写，老库和新库同时写入。注意：任何对数据库的增删改都要双写；对于更新操作，如果新库没有相关记录，需要先从老库查出记录，将更新后的记录写入新库；为了保证写入性能，老库写完后，可以采用消息队列异步写入新库。

利用脚本程序，将某一时间戳之前的老数据迁移到新库。注意：1，时间戳一定要选择开启双写后的时间点，比如开启双写后10分钟的时间点，避免部分老数据被漏掉；2，迁移过程遇到记录冲突直接忽略，因为第2步的更新操作，已经把记录拉到了新库；3，迁移过程一定要记录日志，尤其是错误日志，如果有双写失败的情况，我们可以通过日志恢复数据，以此来保证新老库的数据一致。

第3步完成后，我们还需要通过脚本程序检验数据，看新库数据是否准确以及有没有漏掉的数据

数据校验没问题后，开启双读，起初给新库放少部分流量，新库和老库同时读取。由于延时问题，新库和老库可能会有少量数据记录不一致的情况，所以新库读不到时需要再读一遍老库。然后再逐步将读流量切到新库，相当于灰度上线的过程。遇到问题可以及时把流量切回老库

读流量全部切到新库后，关闭老库写入（可以在代码里加上热配置开关），只写新库

迁移完成，后续可以去掉双写双读相关无用代码。

5.利用数据同步工具，比如Canal、DataBus
利用同步工具，就不需要开启双写了，直接用Canal做增量数据同步即可。
相应的步骤就变成了：

运行Canal代码，开始增量数据（线上产生的新数据）从老库到新库的同步。

利用脚本程序，将某一时间戳之前的老数据迁移到新库。注意：1，时间戳一定要选择开始运行Canal程序后的时间点（比如运行Canal代码后10分钟的时间点），避免部分老数据被漏掉；3，迁移过程一定要记录日志，尤其是错误日志，如果有些记录写入失败，我们可以通过日志恢复数据，以此来保证新老库的数据一致。

第3步完成后，我们还需要通过脚本程序检验数据，看新库数据是否准确以及有没有漏掉的数据

数据校验没问题后，开启双读，起初给新库放少部分流量，新库和老库同时读取。由于延时问题，新库和老库可能会有少量数据记录不一致的情况，所以新库读不到时需要再读一遍老库。逐步将读流量切到新库，相当于灰度上线的过程。遇到问题可以及时把流量切回老库

读流量全部切到新库后，将写入流量切到新库（可以在代码里加上热配置开关。注：由于切换过程Canal程序还在运行，仍然能够获取老库的数据变化并同步到新库，所以切换过程不会导致部分老库数据无法同步新库的情况）

关闭Canal程序
迁移完成。

六、扩容缩容方案
需要对数据重新hash取模，再将原来多个库表的数据写入扩容后的库表中。整体扩容方案和上面的不停机迁移方案基本一致。采用双写或者Canal等数据同步方案都可以。

更好的分库分表方案，不难看出我们的分库分表方案有一些缺陷，比如采用hash取模的方式会产生数据分布不均匀的情况，扩容缩容也非常麻烦。
采用用一致性hash方案解决。基于虚拟节点设计原理的一致性hash可以让数据分布更均匀。

七、降级方案
在大促期间订单服务压力过大时，可以将同步调用改为异步消息队列方式，来减小订单服务压力并提高吞吐量。

1.订单存储到消息队列。
2.消息队列消费者,将订单写入redis,批量10条后写入一次数据库。
3.前端页面下单后定时向后端拉取订单信息，获取到订单信息后跳转到支付页面。

因为订单是异步写入数据库的，就会存在数据库订单和相应库存数据暂时不一致的情况，以及用户下单后不能及时查到订单的情况。因为毕竟是降级方案，可以适当降低用户体验，我们保证数据最终一致即可。