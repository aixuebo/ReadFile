spark执行逻辑
一.视频input模块
split和reader去拆分hdfs或者jdbc等数据源,拆分成多个数据块,方便RDD去执行
二.executor模块---在jvm中执行具体的代码
调度模块产生若干个JVM进程在不同的节点上执行rdd逻辑,每一个rdd读取的一个数据块

1.每一个worker节点都会产生executor,可能一个节点产生若干个executor。
比如我申请spark执行的时候,申请3个executor、每一个executor的内存1G和cpu数量4。那么最终executor不一定会在3个节点上分布,可能分布在1个节点上，只要该节点有足够资源即可。
一个executor是一个JVM进程,里面有线程池去执行业务逻辑,因此spark是基于线程开发的,但是executor是进程级别的,有多少个线程取决于定义worker的cpu数量,每一个cpu都是一个线程,每一个线程执行一个task。
因此上面的demo中,同时可以执行3*4个task任务,每一个任务占用一个cpu
注意:
spark.tasks.cpu=1默认表示一个task需要一个cpu,当一个task需要多个cpu的时候,就需要修改这个
2.worker节点--产生executor---分配task去执行任务(task序列化,可以在节点之间传递即可)
3.workder上工作
创建executor去执行task
将每一个task的执行进度通知给driver
4.worker节点需要知道的属性
driver的url、executorId、worker的host、该executor需要用多少cpu、executor所属applicationId
5.executor
创建一个服务,连接driver,抓去driver上的spark conf配置信息
在本地根据driver上的配置信息,去创建一个本地服务,方便与driver通信
连接driver,向driver注册该executor
接收driver端发送给executor的task任务、kill该executor、kill某一个task
上报给driver端某一个task的进度、心跳

6.task的结果
如果很小,则直接传输给driver
如果较大,则存储在本地worker节点上,产生一个blockManager的blockId,将其发送给driver即可,这样driver要用的时候就会自己下载了
7.多线程的优点
资源共享,所有任务都公用同一套jar包即可


三.调度包---driver上进行调度所有的stage和task
1.spark一个context需要一个队列
因为spark的每一个run方法,即rdd的action行为都会产生一组任务去真的执行
如果多线程环境下产生action行为,就会短期内产生很多任务,所以需要一个队列去维护

2.TaskSetManager 表示每一个action对应的所有任务,即属于同一个stage级别的所有任务

3.每一个stage级别下的任务都在TaskSet中存储

4.每一个task应该在哪个executor、host、rack等节点上执行是固定可以知道的,
当driver的调度器上收到一个空闲的executor时候,就可以将合理的task分配给该executor即可,如何在众多task中找到最应该属于该executor执行的任务,就是根据这些节点属性判断的。

5.当队列上申请到空闲的executor后,会在某一个stage阶段获取一个task任务,发送给executor节点去执行

6.driver的收到某一天task执行完成的数据,通过blockId或者返回值可以得到执行结果
task可能是shuffleTask 或者resultTask,因此shuffleTask就不会有结果到driver上
shuffleTask其实是执行map端的任务,将结果merge后存储在executor节点上
shuffleTask在map端返回的结果是MapStatus对象,里面包含了该结果在哪个executor上执行的,以及结果中为每一个reduceID产生多少大小的结果集
这样driver就知道了,当shuffleTask在reduce上抓去数据的时候,根据reduceId就可以在driver上知道从哪些executor上抓去shuffle的map端数据的,然后在reduce上进行merge



7.driver如何和yarn交互
driver因为启动的时候知道申请多少个executor、以及每一个executor需要多少内存和cpu,因此一旦yarn给与了这些资源后,就不再需要了
每次driver收到5中一个空闲的executor,说明某一个executor的cpu任务执行完了,可以继续获取新的任务了


8.applicationId是不变的,一个sparkContext就对应一个applicationId
每一个action都是一个job,每一个job,即action 因为shuffle等操作,导致会产生很多stage,每一个stage包含了各个task任务(map、reduce)
即一旦遇见shuffleRdd就产生一个stage,而shuffleRdd本身就是在reduce执行的,即reduce的第一个任务就是shuffleRdd的内容,即第一步就是抓去map端的数据



四.广播包
就是传递字节数组,将对象转换成字节数组,然后广播出去,这样其他节点就会可以接收到该字节数组,反序列化成对象



五.shuffle 进行数据merge处理
1.shuffle的map端,每一个reduce产生一个文件.但是文件太多了,因此我们优化成一个executor上,一共就产生reduce个文件.即有多少个reduce就产生多少个文件即可。


六、存储


