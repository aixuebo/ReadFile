一、val conf = new SparkConf().setAppName("Arima_premium_predict").set("spark.executor.instances", "30").set("spark.executor.cores", "1").set("spark.executor.memory", "3g").set("spark.ui.port","4038").set(""spark.driver.memory","2G")
spark.task.cpus 每一个任务需要消耗多少个cpu,因为每一个task都是在executor节点上多线程执行的,默认是一个任务占用一个cpu

表示开启30个executor,每一个executor有一个线程,一个executor一共持有3G内存,ui在端口4038上执行
spark.driver.memory表示driver的内存使用,driver与executor是可以设置不同的内存使用
spark.driver.cores 表示driver使用的cpu数量
详细参数参考与org.apache.spark.deploy.SparkSubmitArguments

val conf = new SparkConf().setAppName("Arima_premium_predict").setMaster("local") 表示本地模式执行
二、spark的rdd如何产生job
1.spark的rdd是一个action行为产生一个job
2.在一个action内,有若干个transfer,而transfer还有分shuffle阶段的,一次shuffle阶段就表示一个stage
三、广播
1.object MyObject extends App { 在这里面的广播是返回null的,不可以被使用
例如
val logDateDelimiter = args(0) //"2017-02-27" //此时程序处理什么时候的日志,带有分隔符
val logDate = DateUtil.convert(logDateDelimiter) //"20170227"
val blogDate = sc.broadcast(logDate)

blogDate.value = null

2.要想被使用,则在main方法中使用
object ArimaPremiumPredictHive {

def main (args: Array[String]) {
	val logDateDelimiter = args(0) //"2017-02-27" //此时程序处理什么时候的日志,带有分隔符
	val logDate = DateUtil.convert(logDateDelimiter) //"20170227"
	val blogDate = sc.broadcast(logDate)
	
	blogDate.value != null
四、不需要序列化的字段
  @transient private var _config: SimpleConsumerConfig = null  //transient关键字表示对该对象不需要进行序列化


五、--queue spark_queue 启动时候加入到哪个队列中

六、启动脚本 以及如何设置动态conf参数
#!/bin/bash

statisticsPath=/server/app/spark_streaming/lib
logPath=/server/logs/spark_streaming

export LIB_PATH=$statisticsPath/*.jar

jarpaths=`echo $LIB_PATH | sed -e 's/ /,/g'`

##echo ---------------------
##echo $jarpaths
spark-submit \
--master yarn_cluster --class com.datacenter.machineLearning.streaming.BinlogTransfer \
--jars $jarpaths \
--verbose \
--conf spark.streaming.kafka.maxRatePerPartition=4500 --conf spark.ui.port=4038 \
--conf spark.streaming.kafka.binlogTableName=cash_record \
/server/app/spark_streaming/job/spark_project.jar $* 1>>$logPath/binlogTransfer_out_test.log 2>>$logPath/binlogTransfer_err_test.log

注意:自定义的conf要想从sc中get到,前缀一定要是spark.streaming.kafka.开头