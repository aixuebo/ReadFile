一、dataframe的数据保存到hive的表中
     val finalResultHive = hiveSqlContext.createDataFrame(finalResult)//注意 一定要用hiveSqlContext去执行
     //追加到hive数据库中
     finalResultHive.registerTempTable("temp")
     finalResultHive.sqlContext.sql("insert overwrite table databases.preference_hour partition(log_type='"+investPathStr+"') select userid,startTime,endTime from temp")
二、从csv文件中加载dataframe
      <dependency>
          <groupId>com.databricks</groupId>
          <artifactId>spark-csv_2.10</artifactId>
          <version>1.4.0</version>
      </dependency>
val filepath1 = "F:\\Project\\20170210_Arima_premium_predict\\samples.csv"//数据内容用逗号拆分,每一行一条数据
val premiumRawData = sqlContext.read.format("com.databricks.spark.csv").option("inferSchema","true").load(filepath1).toDF("userid","premium_final","log_day") //存量数据
三、初始化的一些环境
  val sc = new SparkContext(conf)
  val sqlContext = new SQLContext(sc)
  val hiveSqlContext = new HiveContext(sc) //执行hive
  import sqlContext.implicits._ //可以隐式转换$("属性")
