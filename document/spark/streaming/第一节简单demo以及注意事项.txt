一、demo
import org.apache.spark.SparkConf
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}
import kafka.serializer.StringDecoder

/**
 * 用于从kafka中读取binlog信息,然后对binlog的内容添加统计需要的维度,存储到kafka中
 */
object BinlogTransfer {

  def main (args: Array[String]) {
    val ssc = functionToCreateContext()
    ssc.start()
    ssc.awaitTermination()
  }

  def functionToCreateContext(): StreamingContext = {
    val sparkConf = new SparkConf().setAppName("binlogTransfer").setMaster("local")
      .set("spark.local.dir", "~/tmp")
      .set("spark.streaming.kafka.maxRatePerPartition", "10")
    val ssc = new StreamingContext(sparkConf, Seconds(30))

    val topicsSet = "binlogTransfer".split(",").toSet
    val kafkaParams = scala.collection.immutable.Map[String, String]("metadata.broker.list" -> "ip:port,ip:port","group.id" -> "test2","auto.offset.reset"->"smallest") //auto.offset.reset=largest/smallest
    val directKakfaStream = KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc ,kafkaParams ,topicsSet)

    //读取kafka内的所有数据内容
    directKakfaStream.map(kv => kv._2).foreachRDD( rdd => rdd.collect().foreach(println(_)))

    ssc
  }

二、因为数据可能出现错误,导致重新跑
1.第一种方案是设置每一个topic-partition的位置,从指定位置开始跑
2.第二种解决方案是从最老的数据开始读取,重新恢复数据
a.val kafkaParams = scala.collection.immutable.Map[String, String]("metadata.broker.list" -> "ip:port,ip:port", "auto.offset.reset" -> "smallest","group.id" -> "test2")
b.先删除要重新统计的redis内的统计指标值
c.代码里有参数配置要从哪天开始跑历史数据,到哪天为止
d.循环所有kafka的历史数据,只要大于b参数设置的时间区间的数据都要计算,然后更新redis记录即可

三、限流
    val sparkConf = new SparkConf(). .set("spark.streaming.kafka.maxRatePerPartition", "100")
    val ssc = new StreamingContext(sparkConf, Seconds(30))
上面的配置表示每秒spark streaming只读取每一个kafka的partition只能读取100条数据,即一个批处理30s,则只会读取3000条数据
即如果有3个partition,则每秒能读取9000条数据去处理

另外spark.streaming.receiver.maxRate这个属性限制每秒的最大吞吐。即限制receiover方式的流量
