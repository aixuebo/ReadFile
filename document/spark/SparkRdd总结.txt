

  private[scheduler] def handleJobSubmitted(jobId: Int,

  ------
    def hadoopFile[K, V](
      path: String,
      inputFormatClass: Class[_ <: InputFormat[K, V]],
      keyClass: Class[K],
      valueClass: Class[V],
      minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope {
    assertNotStopped()
    // A Hadoop configuration can be about 10 KB, which is pretty big, so broadcast it.
    //使用java的方式对hadoop的Configuration信息进行序列化与发序列化
    val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration))
    //定义一个函数,参数是JobConf,无返回值,该函数将path写入hadoop的输入源中
    val setInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path)
    new HadoopRDD(
      this,
      confBroadcast,
      Some(setInputPathsFunc),
      inputFormatClass,
      keyClass,
      valueClass,
      minPartitions).setName(path)
  }

def textFile方法解析
1.即给定一个path,使用TextInputFormat类读取该hadoop路径,解析key-value类型为long和Text,使用多少个partition读取  返回HadoopRDD,即RDD[(KEY,VALUE)]
2.调用RDD[(KEY,VALUE)].map(pair => pair._2.toString) 返回MapPartitionsRDD[String] RDD[String] 即对每一个RDD的Partition中key-value的迭代器 循环 只要value的值


细节
1.返回HadoopRDD,即RDD[(KEY,VALUE)]的过程
a.使用InputFormat的getSplits方法切分path路径,返回Split数组
  每一个split数组转换成HadoopPartition对象,即返回HadoopPartition数组,HadoopPartition由RDDID 第几个split 以及split对象组成

------------------------------------------------------------------------------------

测试基本功能的数据
val data = List("hi","hello","how","are","you","hi","hello","how","are","you")
val dataRDD = sc.makeRDD(data,3)
val pipeRDD = dataRDD.zipWithUniqueId()
pipeRDD.collect()


RDD的基本功能
一、Transformations
1.map
def map[U: ClassTag](f: T => U): RDD[U]
方法对该RDD的某一个partition处理,每一个T通过一个函数转换成U,f: T => U,最终是MapPartitionsRDD[U]类型的RDD
2.flatMap
def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U]
方法对该RDD的某一个partition处理,每一个T通过一个函数f: T => TraversableOnce[U] 该函数可以将T转换成一个集合
3.filter
def filter(f: T => Boolean): RDD[T]
方法对该RDD的某一个partition处理,每一个T通过一个函数f: T => Boolean,只有返回true的才允许通过,组成新的RDD[T]
4.distinct 返回一个新的RDD,新的RDD就是把老的RDD中重复的元素去掉了
def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
a.通过map转换成MapPartitionsRDD[(key,null)]元组RDD,该RDD通过隐式转换成PairRDDFunctions对象,该对象持有该RDD
一定需要shuffle,按照key相同的分配到同一个组里面,然后对相同的key对应的value在执行合并函数
5.mapPartitions
a.
def mapPartitions[U: ClassTag](f: Iterator[T] => Iterator[U],
      preservesPartitioning: Boolean = false): RDD[U] {
相当于Map操作,他是一个变种,map输入函数是应用于RDD中每个元素，而mapPartitions的输入函数是应用于每个分区，也就是把每个分区中的内容作为整体来处理的。

将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection，这样开销很大，如果使用mapPartitions，那么只需要针对每一个分区建立一个connection

b.def mapPartitionsWithIndex[U: ClassTag](
        f: (Int, Iterator[T]) => Iterator[U], //f参数int表示第几个partition,iter表示该partition的数据源的迭代器,返回新的迭代器
        preservesPartitioning: Boolean = false): RDD[U]
也是mapPartitions的升级版本,就是参数不仅仅是需要partition的内容的迭代器,还需要知道这个任务是第几个任务,即map的ID

6.  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] 调用 coalesce(numPartitions, shuffle = true)
  就是简单的将父RDD的多个partition合并成一个partition的过程,虽然有网络获取数据的内容,但是不涉及真正意义上的shuffle操作,即不涉及key相同的都要在一个节点上存在步骤
7.def coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null) : RDD[T]
参数shuffle = true表示要进行一次shuffe操作去重新设置partition
   如果shuffle = false 表示不会进行shuffle,只是一对多的,将多个父RDD的partition合并成一个,虽然有网络IO,但是不涉及key相同的都要在一个节点上存在步骤
参数numPartitions 表示最终需要的partition数量
该函数是将父RDD的partition数量重新调整,一般用于RDD的partition较多又较小的时候,重新规划

注意:
shuffle true的时候,让每一个原始的RDD的每一个partition产生一个新的key,key的内容就是自增长的ID,,让shuffle根据id作为key的时候更均匀的分布在多个节点上,依然不用保证key相同的都在同一个节点上

8.def union(other: RDD[T]): RDD[T] 或者 def ++(other: RDD[T]): RDD[T]
将两个RDD进行合并，不去重
9.def intersection(other: RDD[T]): RDD[T]
函数返回两个RDD的交集，并且去重。
将两个集合的key转换成<key,null>这样K-V结构的就可以调用的PairRDDFunctions里面<K,V>结构的cogroup逻辑,这样得到的同一个key的两个结果集都存在的,则就是需要的交集
10.def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)]
对两个RDD进行笛卡尔计算,返回RDD[(T, U)],这个函数对内存消耗较大,使用时候需要谨慎
即两个RDD每一条记录都相互笛卡尔运算
11.def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]
将RDD中的T元素通过函数转换成另外对象,然后组成<K,T>的元组,从而使用PairRDDFunctions的groupBy,将数据按照K进行分组
12.def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]
将两个RDD每一个元素拿出来,作为元组
比如
scala> var rdd1 = sc.makeRDD(1 to 5,2)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at :21

scala> var rdd2 = sc.makeRDD(Seq("A","B","C","D","E"),2)
rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at makeRDD at :21

scala> rdd1.zip(rdd2).collect
res0: Array[(Int, String)] = Array((1,A), (2,B), (3,C), (4,D), (5,E))

13.def keyBy[K](f: T => K): RDD[(K, T)]
将元素T通过函数转换成K,组装成K,T的元组

14.def sortBy[K](
         f: (T) => K,
         ascending: Boolean = true,
         numPartitions: Int = this.partitions.length)
         (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]
按照函数f对元素进行排序,返回的RDD还是原来的RDD,只是进行了一次按照f函数来排序的过程
该函数会产生一个shuffle操作
将RDD[T]转换成RDD[K,T],然后因为K是有排序功能的,因此shuffle按照K排序,排序后获取所有排序后的value集合
注意实现逻辑:
该RDD的partition,采用抽样的方式,根据抽样的key的排列顺序,划分了key的partition区间,从而保证了key会分配到对应的排序好的partition中,即最终的每一个partition都是有顺序的,
shuffle又设置Key的排序方式,shuffle又保证了每一个partition内部又有顺序,因此最终就是按照key排号顺序的,不需要map-reduce中最终一个reduce进行排序方案了

15.def glom(): RDD[Array[T]]
每一个分区内所有元素都进入到一个数组中
函数是将RDD中每一个分区中类型为T的元素转换成Array[T]，这样每一个分区就只有一个数组元素。
实现逻辑:
因为每一个partition进入函数的时候都是一个迭代器,因此将迭代器转换成数组Array[T]即可

16.def pipe(
         command: Seq[String],
         env: Map[String, String] = Map(),
         printPipeContext: (String => Unit) => Unit = null,
         printRDDElement: (T, String => Unit) => Unit = null,
         separateWorkingDir: Boolean = false): RDD[String]
类似与hadoop的streaming方式,让非java和scala的用户也可以使用集群并行的运算数据
只要服务器上的语言可以支持从标准输入读取数据--运算--输出字符串到标准输出 这种模式,都可以参与到并行计算里面来
参数说明:
a.参数提供运行命令(脚本以及参数),脚本内容中包含从标准输入中不断循环读取数据即可
b.运行环境变量
c.printPipeContext 提供一个函数,可以在每一个executor运行该脚本前向输出内打印一些信息
d.printRDDElement 提供一个函数将原始的RDD元素转换成一种String类型,然后标准输入处理转换后的数据
例如:
例如:groupBy之后的数据作为数据流传入到pipe中,格式是record:(String, Seq[String])的,但是很不容易让后续脚本操作,最好脚本操作的都是String类型的,
   因此代替方案是,将属于一个key的集合转换成String类型,然后在将String类型调用f函数进行输出到输入流中

具体demo详细查看PipedRDD的备注以及回家后的视频

17.def randomSplit(
  weights: Array[Double],
  seed: Long = Utils.random.nextLong): Array[RDD[T]]
   根据权重,数组weights长度是多少,则就对RDD循环抽样多少次,返回RDD数组,每一个数组内容就是抽样的结果RDD
   常用于从原始数据中抽出多少比例的训练集合和测试集合
18.def takeSample(
         withReplacement: Boolean,
         num: Int,
         seed: Long = Utils.random.nextLong): Array[T]
抽样获取num个元素
19.def sample(
         withReplacement: Boolean,
         fraction: Double,
         seed: Long = Utils.random.nextLong): RDD[T]
抽样获取元素


Actions
1.def collect(): Array[T] 和 def toArray(): Array[T]
返回所有运行的结果集,收集每一个partition的结果,每一个partiton的结果都是数组,然后组装成一个大的数组

2.def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U]
偏函数作为参数,可以将每一个元素转换成U,不符合参数T的元素可以自动忽略掉

def collectPartitions(): Array[Array[T]]
返回每一个partition的迭代器集合,即如果RDD有5个partition,则返回数组是5个,其中每一个元素又是一个迭代器,因此是数组套数组

3.def toLocalIterator: Iterator[T]
将全部的数据返回到本地,返回的结果是元素的迭代器

4.def foreach(f: T => Unit): Unit
每一个partition的一行内容,即T,被调用函数f进行处理,可以在f函数中进行打印信息操作

5.def foreachPartition(f: Iterator[T] => Unit): Unit
每一个partition的所有内容,即Iterator[T],被调用函数f进行处理,可以在f函数中进行打印信息操作

6.def reduce(f: (T, T) => T): T
将每一个partition的数据,进行运算,返回一个T对象,然后每一个partition的结果T再参与运算.最终返回一个T
即RDD所有的元素进行f运算,返回一个值

def treeReduce(f: (T, T) => T, depth: Int = 2): T
会调用treeAggregate方法,参见treeAggregate方法

7.def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U
聚合,将每一个元素的值进行集合,产生新的元素U,有初始化值

因为aggregate和reduce都是每一个partition去聚合,然后所有partition的数据到driver上再进行最终合并的
因此如果partition数量很大的话,会造成在driver上的合并会占内存,甚至内存溢出,因此应该让到达driver的partition数量少一些

因此就有了treeAggregate方法
def treeAggregate[U: ClassTag](zeroValue: U)(
      seqOp: (U, T) => U,
      combOp: (U, U) => U,
      depth: Int = 2): U
他会在原始RDD上重新根据depth深度.重新规划partition数量
比如原始partition有100个,要3层,因此首先计算100/3,让partition变成33,然后每一个partition就变成(partitionId % 33,原始记录)这样的元组,进行一次reduceByKey
这样循环3次,已达到最终driver的数据会合并的很少

8.def fold(zeroValue: T)(op: (T, T) => T): T
每一个partition都从zeroValue开始初始化,剩下的与reduce相同

9.def count(): Long
每一个partition计算有多少条数据,在driver中汇总所有数据的条数

10.def max()(implicit ord: Ordering[T]): T 和 def min()(implicit ord: Ordering[T]): T
找到RDD中最小和最大的元素
前提是RDD的元素内容是可以比较的,实现方式是调用的reduce方法,找到最小的和最大的

11.def take(num: Int): Array[T]
获取前num个元素,这些元素是从第0个partition开始获取,一直获取到num数量为止,获取的数据跟排序没关系

12.def first(): T
获取第一个元素,该第一个元素不是排序后的第一个元素,而就是随意产生的一个元素
实现原理take(1)

13.def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]
获取所有RDD中排序后前num个元素,默认的结果的元素是从小到大排序的排序
实现原理
a.每一个partition都有一个队列,都从队列中获取num个元素
b.合并每一个队列,但是最终合并后的队列也是num个元素

14.def top(num: Int)(implicit ord: Ordering[T]): Array[T]
该方法是takeOrdered的反方法,是从大到小的获取前num条数据

15.def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long]
获取每一个值出现的次数,即1出现4次..
实现原理map(value => (value, null)).countByKey()

16.def zipWithUniqueId(): RDD[(T, Long)]
为每一个元素添加一个唯一的序号,最后组成元组

17.def zipWithIndex(): RDD[(T, Long)]
为每一个元素产生一个唯一的序号
该方式不太好,因为他要先扫描所有的partition元素,确定每一个partition中有多少个元素,然后设置每一个partition序号的最小值,然后算出来的序号
比如3个partition,第一个partition有3个记录,第二个partition有5条记录,第三个partition有2条记录
因此第一个partition的序号就是0 1 2 第二个partition序号就是3 4 5 6 7 第三个partition序号就是8 9

18.def subtract(other: RDD[T]): RDD[T]
返回在RDD中出现，并且不在otherRDD中出现的元素，不去重
将x变成(x,null) 这样K-V结构的就可以调用的PairRDDFunctions里面<K,V>结构的subtract逻辑


saveFile
1.def saveAsTextFile(path: String): Unit
循环所有的partition元素.每一个元素组装成元组(NullWritable.get(),new Text()),然后存储到Hadoop的path下,存储格式是TextOutputFormat[NullWritable, Text]
2.def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit
与1方法一样,只是保存的内容不是纯文本,而是使用压缩算法压缩过的内容
3.def saveAsObjectFile(path: String): Unit
循环所有的partition元素.每一个元素组装成元组(NullWritable.get(),new BytesWritable(Utils.serialize(元素内容))),存储到hadoop的path下,存储格式是SequenceFileOutputFormat

4.persist
def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type
def persist(newLevel: StorageLevel): this.type
def cache() =  def persist() = persist(StorageLevel.MEMORY_ONLY)
def unpersist(blocking: Boolean = true): this.type
暂时不理解如何存真正储RDD的

5.def checkpoint(): Unit
将RDD的内容写入到hdfs上,路径是固定路径/rddId/partitionid对应的文件,即每一个partitionid对应一个文件

checkpoint与persist区别
cache和persist都是Spark用来缓存数据的方法,
cache和persist的唯一区别就是cache的存储级别是MEMORY_ONLY，而persist可以指定存储级别
cache&persist和checkpoint的区别在于cache&persist只是为了加快数据的计算，RDD的依赖链路是完整保存的，在cache&persist的数据不可用时，Spark还可以根据RDD的依赖关系重新计算得到数据。
而checkpoint检查点对应的RDD的依赖链路是不保存的，对应的RDD只有一个指向检查点文件的父RDD，所以在检查点数据丢失的情况下，Spark是无法根据RDD的依赖链路恢复数据的。
而且由于cache&persist的数据由BlockManager管理，所以在driver程序执行结束时，被cache&persist的数据也会被清空。而checkpoint的数据是写入诸如HDFS文件系统中的，是独立存在的，所以可以被下一个driver程序执行使用。



隐式转换
1.可以将RDD[(K, V)]转换成PairRDDFunctions(rdd),即对KEY-value的RDD增加一些新的功能函数,该函数由PairRDDFunctions类提供
2.可以将RDD[(K, V)]转换成OrderedRDDFunctions(rdd),条件是Key是支持排序的Key,即对KEY-value的RDD增加一些新的功能函数,该函数由OrderedRDDFunctions类提供
3.可以将RDD[(K, V)]转换成SequenceFileRDDFunctions(rdd),条件是Key和value都是支持hadoop的Writable接口的,即对KEY-value的RDD增加一些新的功能函数,该函数由SequenceFileRDDFunctions类提供
4.可以将double的RDD,RDD[Double]转换成DoubleRDDFunctions,为double运算提供了新的函数
5.可以将RDD[T]元素是Numeric数字类型的都转换成DoubleRDDFunctions,为数字运算提供了新的函数,实现是将每一个数字map成double类型
6.可以为每一个RDD都转换成AsyncRDDActions,为RDD本身提供了异步的功能函数


PartitionPruningRDD
对父RDD的partition进行精简筛选
函数partitionFilterFunc表示 传入一个partition的id,返回该partition是否要被筛选掉,返回true的表示是最终要保留的partition


1.PartitionerAwareUnionRDD 表示把N个RDD,每一个partition序号相同的,进行合并,即如果有10个partition,则最终结果依然是10个partition,只不过每一个partiton存在了更多的RDD的数据而已
2.UnionPartition 表示将N个RDD进行合并,称为更大的RDD,但是新的RDD的partition数量是N个RDD的partition数量之和,即不会合并RDD处理
3.SubtractedRDD 对两个RDD进行运算,找到在第一个RDD中存在的key,但是在第二个RDD中不存在的数据
该实现逻辑是 如果两个RDD的partitoner不同,则要先进行shuffle,确保同一个key都在同一个partition中,然后才能方便判断是否存在Key
而在同一个partition中,利用内存方式判断是否存在,因此可能会造成内存益处的可能,但是也不会,因为一个partition的内容都已经装进来了,而一个文件数据块又128M,因此也不会造成内存溢出,但是至少该方法一定是耗费内存的方法
4.CoGroupedRDD[K](@transient var rdds: Seq[RDD[_ <: Product2[K, _]]], part: Partitioner)
  extends RDD[(K, Array[Iterable[_]])](rdds.head.context, Nil)
传入一组RDD[(k,v)],然后对该RDD按照Key分组,返回同一个Key对应每一个RDD的迭代器
返回值是RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2], Iterable[W3]))],即key与三个RDD对应的迭代器组成的元组作为value返回
该实现逻辑是 如果多个RDD的partitoner不同,则要先进行shuffle,确保同一个key都在同一个partition中,然后才能方便判断是否存在Key
肯定耗费内存,因此可能内存溢出,因此该方法默认是支持向磁盘写入和排序的
返回的迭代器中V是没有顺序的,是乱序的
5.CartesianPartition
两个RDD进行笛卡尔乘积运算,即第一个RDD的每一个元素都与第二个RDD的每一个元素进行两两运算,产生元组[Pair[T, U]]
6.OrderedRDDFunctions
对RDD[K,V]形式的数据,其中K是支持排序的隐式转换,提供一些附加功能
a.def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length) : RDD[(K, V)]
对RDD[K,V]结构的数据进行排序,按照K进行排序,返回排序后的RDD[K,V]
该算法有一些运算成本,因为先会扫描一下RDD,采用抽样的方式,将采取回来的数据按照key划分了若干个partition区间,从而保证了key会分配到对应的排序好的partition中,即最终的每一个partition都是有顺序的,
而shuffle阶段又保证了同一个partition的数据在shuffle的时候又有顺序,因此保证了全局的有序性
b.def repartitionAndSortWithinPartitions(partitioner: Partitioner): RDD[(K, V)]
重新分配partition数量,并且保证每一个shuffle的结果是有顺序的,但是不保证多个partition文件中key是有顺序的
c.def filterByRange(lower: K, upper: K): RDD[P]
查找全部RDD中的一个子集,子集的内容是key在[lower,upper]区间即可
设置K的区间范围,即给定该区间的K的最小值和最大值

7.CoalescedRDD
代表一个合并RDD,合并后的partition比合并前的要少很多,即新的RDD中一个partition会读取父RDD中若干个partition信息即可
难度在于如何实现的合并划分算法,有时间可以详细看一下视频,因为这个划分算法还是有一定难度的


------------------------------------------------------------------------------------
PairRDDFunctions(持有K-V元组形式的RDD作为私有属性,一切RDD操作都是操作该K-V类型的RDD)
1.combineByKey[C](createCombiner: V => C,//可以value转换成C的函数
      mergeValue: (C, V) => C,//对每一个C与V交互生成C的函数
      mergeCombiners: (C, C) => C,//将多个C进行合并的函数
该方法一定发生shuffle了
对key不存在的,走第一个参数,让key转换成c
对key存在的,则找到对应的C,然后与此时的key对应的value进行运算,走第二个参数
第三个参数相当于reduce操作
2.def aggregateByKey[U: ClassTag](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) => U,
      combOp: (U, U) => U): RDD[(K, U)] = self.withScope {
该方法与combineByKey方法类似,只是第一次出现key的时候,不需要进行key对应的value转换成C,而是用初始化的值zeroValue与此时的value进行合并,即调用combineByKey的第二个参数
3.def reduceByKey(partitioner: Partitioner, func: (V, V) => V): RDD[(K, V)]
按照key进行group by,key相同的进行合并,最终调用的还是combineByKey方法
4.def foldByKey(
      zeroValue: V,
      partitioner: Partitioner)(func: (V, V) => V): RDD[(K, V)]
与reduceByKey一样,都是对V进行处理,返回值还是V,只是区别在于.每一个key的第一个value,要与初始值一起参与F运算,而reduceByKey第一次出现的V就直接返回V
5.def reduceByKeyLocally(func: (V, V) => V): Map[K, V]
在一个partition节点上,将一个partition的迭代器作为参数,在内存中计算,对key进行分组,相同key的value进行func函数运算,因此有内存问题。
如果key很多的时候,会真用很大内存空间。
6.mapValues
def mapValues[U](f: V => U): RDD[(K, U)]
将将RDD[K-V]转换成RDD[k,f[V=>U]] = RDD[k,U]操作,
即对value中值进行map映射,key保持不变
7.flatMapValues
def flatMapValues[U](f: V => TraversableOnce[U]): RDD[(K, U)]
针对RDD[(k,v)] 将其转换成RDD[(K, U)]
也是针对每一个value进行flatmap映射,转换成一个集合TraversableOnce[U]
8.countByKey
def countByKey(): Map[K, Long]
对每一个key计数,返回最终每一个key出现多少次
实现:self.mapValues(_ => 1L).reduceByKey(_ + _).collect().toMap
9.def partitionBy(partitioner: Partitioner): RDD[(K, V)]
重新对RDD进行partition分组,即shuffle环节
10.groupByKey
groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]
按照key分组,然后每一个组内的所有元素组成迭代器集合,和combine区别是,没有对组内相同元素做运算,而是仅仅简单的将相同的元素进行集合收集。
11.cogroup与groupWith是同一个函数,只是groupWith是别名而已
def cogroup[W1, W2, W3](other1: RDD[(K, W1)],
      other2: RDD[(K, W2)],
      other3: RDD[(K, W3)],
      partitioner: Partitioner)
      : RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2], Iterable[W3]))]
RDD本身与其他三个RDD进行各种join操作,按照相同的key进行分组。
返回值k就是相同的key,value就是四个RDD在相同key后的四个集合,这四个集合可以做各种笛卡尔乘积
12.def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]
仅仅操作两张表关联,调用cogroup方法,然后对相同key的两个集合进行笛卡尔乘积
正常情况下应该是一对多关系的两个表

同理 左关联
leftOuterJoin
def leftOuterJoin[W](
      other: RDD[(K, W)],
      partitioner: Partitioner): RDD[(K, (V, Option[W]))]
右关联
def rightOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner)
      : RDD[(K, (Option[V], W))]
全表关联
def fullOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner)
      : RDD[(K, (Option[V], Option[W]))]
都是调用cogroup方法,然后对相同key的两个集合进行相关联的笛卡尔乘积运算

13.def collectAsMap(): Map[K, V]
对RDD[K,V]进行action行为collect调用,收集好的数据转换成Map<K,V>

14.lookup(key: K)
def lookup(key: K): Seq[V]
找到该key对应的所有value值集合
这个会出发action行为,如果定义了如何根据key拆分不同的partition,则直接到指定partition中查找该key对应的value集合即可

15.SubtractedRDD
def subtractByKey[W: ClassTag](other: RDD[(K, W)]): RDD[(K, V)]
对两个rdd进行交互,获取RDD1存在,RDD2不存在数据,即差异的元素

16.saveAsHadoopFile
  def saveAsHadoopFile(
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ <: OutputFormat[_, _]],
      conf: JobConf = new JobConf(self.context.hadoopConfiguration),
      codec: Option[Class[_ <: CompressionCodec]] = None): Unit
将RDD的内容写入到hdfs上,因为这个RDD是K-V类型的RDD,因此只要指定outputFormatClass即可,这类型是满足key-value结构的。
同时指定path和压缩方式
实现逻辑:
执行RDD的action,RDD的每一个partition分区就会写入到hdfs对应的path路径下,因为path已知,partition也已知,因此根据这两个就可以知道一个文件的路径。
因此在hdfs上创建该文件的输出流,就可以向里面写入数据了

17.基础方法
  /**
   * 返回所有的key组成的RDD
   */
  def keys: RDD[K] = self.map(_._1)

  /**
   * 返回所有的value组成的RDD
   */
  def values: RDD[V] = self.map(_._2)

  //key对应的class
  private[spark] def keyClass: Class[_] = kt.runtimeClass

  //VALUE对应的class
  private[spark] def valueClass: Class[_] = vt.runtimeClass

  //key对应的排序对象算法
  private[spark] def keyOrdering: Option[Ordering[K]] = Option(ord)

18.未看懂并且也并不是很常用的函数,留着后续看
a.countApproxDistinctByKey
  def countApproxDistinctByKey(relativeSD: Double,numPartitions: Int): RDD[(K, Long)]
返回RDD中每一个key对应的value的不重复的次数的近似值
该方法调用的第三方实现的,具体实现算法就是HyperLogLogPlus算法
b.countByKeyApprox
def countByKeyApprox(timeout: Long, confidence: Double = 0.95) : PartialResult[Map[K, BoundedDouble]]
对K-V的信息仅抽取K,然后对K计算出现的次数,
即对每一个key计数,返回最终每一个key出现多少次,类似def countByKey(): Map[K, Long]
c.sampleByKey
 def sampleByKey(withReplacement: Boolean,
      fractions: Map[K, Double],
      seed: Long = Utils.random.nextLong): RDD[(K, V)]
抽样返回RDD的一个子集,用于抽样查看相关逻辑


------------------------------
Streaming
一、ReceivedBlockTracker
主要操作三个事件
1.接收到数据信息ReceivedBlockInfo,将ReceivedBlockInfo信息追加到streamId对应的队列中,此时已经表示数据块已经接收完成,但是还没有进行streaming处理
该对象表示一个streamingId,接收到了一个数据块,该数据块有多少条数据,以及一些元数据备注信息
private[streaming] case class ReceivedBlockInfo(
    streamId: Int,//哪个stream
    numRecords: Option[Long],//该数据块有多少条数据
    metadataOption: Option[Any],//该数据块的元数据信息内容.比如是kafka的话,会有topic-partition以及offset等信息
    blockStoreResult: ReceivedBlockStoreResult //存储后的结果

二、定期从队列中将每一个streamId对应的ReceivedBlockInfo集合 提取出来
三、定期清理二中的数据,防止浪费内存

------------------------------
Streaming的recerver包
一、RateLimiter(conf: SparkConf) 流量控制器
因为接收数据的时候可能流量过大,防止流量过大,因此使用该类可以控制流量
1.def getCurrentLimit: Long = rateLimiter.getRate.toLong 获取当前流量
2.def updateRate(newRate: Long) 更新流量,但是不能超过最大流量值
3.def waitToPush() 每次插入数据的时候都调用该方法,该方法会当超速流量的时候阻塞住,该方法一旦返回,说明可以继续接收数据

二、BlockGenerator 数据块产生器
  因为数据是一行一行接收的,而存储的文件是按照一个数据块为单位存储的,即一个数据块应该包含多条数据,因此就有该类用于在内存收集数据,然后定期将收集的数据写入到数据块文件中。
  改善:
  1.该类是定期的收集数据,如果定期周期比较小,数据产生慢,因此会产生很多小的数据块,因此应该再加入一个最小的条数,达到该条数再收集数据。
  2.BlockGeneratorListener 该类持有一个监听器,用于监听一些事情
  def onAddData(data: Any, metadata: Any) 当一条数据或者一组数据添加后,调用该监听,第一个参数是数据内容,第二个参数是一些备注信息
  def onGenerateBlock(blockId: StreamBlockId) 当数据积攒一定程度后,产生一个数据块时候调用该函数,即通知已经产生了一个数据块
  def onPushBlock(blockId: StreamBlockId, arrayBuffer: ArrayBuffer[_]) 将产生的数据块准备写入到磁盘时候调用该方法,该方法保存数据块ID以及数据块内容
  def onError(message: String, throwable: Throwable) 收集过程中出现异常时候调用
  3.def addData(data: Any) 仅仅单独添加数据,将数据添加到内存数组中,作为收集接收到的数据  要检查流量
  4.def addDataWithCallback(data: Any, metadata: Any) 收集数据到数组中,并且调用监听器listener.onAddData(data, metadata) 要检查流量
  5.def addMultipleDataWithCallback(dataIterator: Iterator[Any], metadata: Any)  收集数据到数组中,并且调用监听器listener.onAddData(data, metadata) 要检查流量
  6.每隔一定周期,即200ms时候调用updateCurrentBuffer方法,进行切换数据块,产生一个新的数据块,将收集的数据都附属给该数据块
    产生新的数据块内容,仅仅是包含数据块ID和数据块内容,但是没有写入到磁盘,因此将这些信息存储在队列中
    调用listener.onGenerateBlock(blockId)
  7.blockPushingThread 有一个线程,不断的从队列中获取已经准备好的数据块进行处理,没有数据块则阻塞,获取到数据块后,调用listener.onPushBlock(block.id, block.buffer)

三、ReceiverSupervisorImpl 中实现了BlockGeneratorListener
  private val defaultBlockGeneratorListener = new BlockGeneratorListener {
      def onAddData(data: Any, metadata: Any): Unit = { }

      def onGenerateBlock(blockId: StreamBlockId): Unit = { }

      def onError(message: String, throwable: Throwable) {
        reportError(message, throwable)
      }

      def onPushBlock(blockId: StreamBlockId, arrayBuffer: ArrayBuffer[_]) {
        pushArrayBuffer(arrayBuffer, None, Some(blockId))
      }
    }
四、Receiver
当了解了如何对产生的数据信息收集,以及如何产生一个新的数据块,并且写入到具体文件中后,我们来看一下 接收数据的接口是如何定义的
每一个streaming都要有一个数据入口,这个类就是数据接收器,接收的每一行数据的类型都是T
Receiver[T](val storageLevel: StorageLevel) extends Serializable
1.def onStart()  def onStop() 打开和关闭一个接收器,即如何开始接收数据以及结束接收数据
2.def restart(message: String) ---- supervisor.restartReceiver(message) 如何重新开启一个接收器
3.def preferredLocation : Option[String] 建议在哪个节点上执行该接收器
4.def store(dataItem: T) 如何存储一条数据 supervisor.pushSingle(dataItem)
5.如何存储一组数据
def store(dataBuffer: ArrayBuffer[T]) ---- supervisor.pushArrayBuffer(dataBuffer, None, None)
def store(dataBuffer: ArrayBuffer[T], metadata: Any) ----- supervisor.pushArrayBuffer(dataBuffer, Some(metadata), None)
def store(dataIterator: Iterator[T]) ---- supervisor.pushIterator(dataIterator, None, None)
def store(dataIterator: java.util.Iterator[T], metadata: Any) --- supervisor.pushIterator(dataIterator, Some(metadata), None)
def store(dataIterator: java.util.Iterator[T]) ---- supervisor.pushIterator(dataIterator, None, None)
def store(dataIterator: Iterator[T], metadata: Any) ---- supervisor.pushIterator(dataIterator, Some(metadata), None)
def store(bytes: ByteBuffer) supervisor.pushBytes(bytes, None, None)
def store(bytes: ByteBuffer, metadata: Any) supervisor.pushBytes(bytes, Some(metadata), None)
6.def reportError(message: String, throwable: Throwable) ---- supervisor.reportError(message, throwable) 如何报告异常
7.def streamId: Int = id 该接收器属于哪个streaming
8.supervisor ReceiverSupervisor 接收器真正的工作者,即如何存储数据的,如何停止的,如果处理异常的

五、ReceiverSupervisor 如何处理接收到的数据的抽象类
一个接收器Receiver 对应一个该对象,是该对象启动和停止了Receiver
1.futureExecutionContext 内部一个线程池,最多允许128个线程
2.存储数据的抽象方法
def pushSingle(data: Any)
def pushBytes(
bytes: ByteBuffer,//数据内容
optionalMetadata: Option[Any],//数据的元数据--附加的一些信息
optionalBlockId: Option[StreamBlockId]//数据存储在哪个数据块
)
def pushIterator(
iterator: Iterator[_],
optionalMetadata: Option[Any],
optionalBlockId: Option[StreamBlockId]
)
def pushArrayBuffer(
arrayBuffer: ArrayBuffer[_],
optionalMetadata: Option[Any],
optionalBlockId: Option[StreamBlockId]
)
3.def reportError(message: String, throwable: Throwable) 报告错误
4.def start()  def stop(message: String, error: Option[Throwable])
5. def createBlockGenerator(blockGeneratorListener: BlockGeneratorListener): BlockGenerator  传入一个监听器,创建一个数据块产生器
六、ReceiverSupervisorImpl 具体的接收器实现类
1.onStart() 和 onStop(message: String, error: Option[Throwable])
对内部的数据块产生器进行start和stop方法处理
2.def onReceiverStart() 和 def onReceiverStop(message: String, error: Option[Throwable])
对调度包下的ReceiverTracker进行socket通信,发送receiver已经启动了以及已经停止了,即注册和取消注册一个receiver
3.val registeredBlockGenerators = new mutable.ArrayBuffer[BlockGenerator]
对该receiver上产生数据块行为进行事件监听的一组集合.可以多个监听一起监听该事件
4.def nextBlockId = StreamBlockId(streamId, newBlockId.getAndIncrement) 创建新的数据块
一个receiver有多个数据块,数据块每隔一定周期就产生一个,但是他们有相同的前缀,即streamId,因为一个receiver就是一个streamId
5.def pushSingle(data: Any) 只是将一条该数据加入到内存中
6.def pushAndReportBlock(
      receivedBlock: ReceivedBlock,
      metadataOption: Option[Any],
      blockIdOption: Option[StreamBlockId]
    )
当添加一组信息的时候,将这些信息组装成一个数据块,真正意义去存储到磁盘上
然后向调度包下的ReceiverTracker进行socket通信,发送AddBlock(blockInfo) 即当一个数据块进入磁盘后,通知其他服务说你可以操作这个数据块了
blockInfo 包含 哪个streaming以及插入多少条数据以及对应的数据块ID
7.def reportError(message: String, error: Throwable) 发送错误信息,向调度包下的ReceiverTracker进行socket通信,发送ReportError对象
8.上面的6说真正将数据块的数据写入到磁盘了,那么如何写入的呢?
WriteAheadLogBasedBlockHandler 或者 BlockManagerBasedBlockHandler 进行真正意义的存储数据
9.本地开启服务,让调度包下的ReceiverTracker发送信息,进行删除一些已经没意义的数据、停止该receiver以及更新流量速度
可以接收的信息参见ReceiverMessage类提供的接口

七、ReceivedBlockHandler 用于真正意义的存储数据块的数据到磁盘上
1.ReceivedBlockHandler 接口
def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): ReceivedBlockStoreResult 存储该数据块ID以及数据块ID对应的内容,返回存储结果
def cleanupOldBlocks(threshTime: Long) 清理过期的数据
2.ReceivedBlockStoreResult 保存结果的接口
def blockId: StreamBlockId 数据块ID,即属于哪个数据块的保存结果
def numRecords: Option[Long] 该数据块内保存了多少条数据
3.高级的保存结果的接口 WriteAheadLogBasedStoreResult
不仅存储了数据块ID以及多少条数据,还多加了WriteAheadLogRecordHandle对象,用于知道日志的位置
4.BlockManagerBasedBlockHandler磁盘存储器,将结果存储到磁盘上
调用BlockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel)方法将数据属于的blockId以及内容迭代器添加到存储器中,存储的级别取决于参数StorageLevel
不支持cleanupOldBlocks删除方法
5.WriteAheadLogBasedBlockHandler 保存在存储器上,同时也保存在日志中做备份
第一部分:保存
a.调用blockManager.dataSerialize将数据块内容序列化成字节数组
b.调用blockManager.putBytes 将字节数组添加到存储级别中
  同时调用writeAheadLog.write(serializedBlock, clock.getTimeMillis()) 将字节数组添加到日志中,并且返回WriteAheadLogRecordHandle对象
c.将数据块ID以及该数据块内包含多少条数据以及日志的返回值WriteAheadLogRecordHandle组装成WriteAheadLogBasedStoreResult 结果返回
第二部分 清理老数据
cleanupOldBlocks 调用writeAheadLog.clean(threshTime, false),清理日志中的过期数据
第三部分 细节
a.private val writeAheadLog = WriteAheadLogUtils.createLogForReceiver(conf, checkpointDirToLogDir(checkpointDir, streamId), hadoopConf) 创建日志对象














