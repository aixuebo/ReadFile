1.spark的map等链式操作比纯粹的java操作要清晰,但是在try异常处理方面不如java好
2.spark的RDD的cache可以在内存中存储中间结果,比hadoop每次从文件系统读取要好很多
3.spark的第2条不是让他快的原因,他快的原因是spark的中间结果可以单独保存,然后下面用到他的程序不需要计算,直接获取最终结果即可。
避免每次都重复从头计算一遍的痛苦
4.spark代码开发比hadoop要容易，hadoop每次一个mr接下一个mr的时候,都要写入输入、输出等参数,spark可以通过链路方式,抽象RDD可以很容易解决了操作易用性这方面的问题
5.spark是线程级别的,而hadoop的进程级别的,每一个mr都是一个单独的进程
通过看org.apache.spark.executor.Executor这个类
可以知道spark的进程级别的,即一个Executors是一个进程,而该进程内所有的Task任务都是多线程执行的。
因此只要将所有的代码封装到org.apache.spark.scheduler.Task中,因此持有该jar包的任意程序都可以执行Task的run方法,因此至于是多线程还是多进程持续都无所谓了,这样的架构非常漂亮
还有好处就是不仅节省了JVM的创建、销毁时间问题,而且多个Task需要依赖的相同的文件和jar包只需要下载一次即可。
同时多线程使用线程池控制每一个线程Task的run,让其异常可以被catch住,从而一个任务失败不影响其他任务(同一个进程内的),完美
