一、个人感觉spark、flink都没有设计好，没有参照hadoop的逻辑，最终造成各种OOM。

1.shuffle前，数据按照顺序存储在磁盘上。
2.shuffle后，reduce节点去各个磁盘拉数据该reduce的数据，得到N个文件，本身文件是有顺序的，只需要内部做并归排序就可以了，不耗费内存。
3.如果是join操作，reduce节点拉去不同流的数据，比如得到2N文件，每一个流内部做并归排序就可以了。


如果针对二次排序的需求，只需要reduce阶段根据key已经排序后的数据，再一次提取key即可。可参考hadoop的二次排序方案。


spark设计缺陷的原因:
1.reduce阶段对所有的数据排序并不是强需求，不是所有数据都需要排序好的，强行排序会造成负担。
Spark Map输出的数据没有经过排序，Spark Shuffle过来的数据也不会进行排序，Spark认为Shuffle过程中的排序不是必须的，并不是所有类型的Reduce需要的数据都需要排序，强制地进行排序只会增加Shuffle的负担。
所以reduce拿到的数据并不是排序好的数据。

2.Reduce拖过来的数据会放在一个HashMap<key, List<value>>中，Spark将Shuffle取过来的每一个<key, value>对插入或者更新到HashMap中，来一个处理一个。HashMap全部放在内存中。

Shuffle取过来的数据全部存放在内存中，对于数据量比较小或者已经在Map端做过合并处理的Shuffle数据，占用内存空间不会太大。

3.但是对于比如group by key这样的操作，Reduce需要得到key对应的所有value，并将这些value组一个数组放在内存中，这样当数据量较大时，就需要较多内存。
当内存不够时，要不就失败，要不就用老办法把内存中的数据移到磁盘上放着。
Spark意识到在处理数据规模远远大于内存空间时所带来的不足，引入了一个具有外部排序的方案。
Shuffle过来的数据先放在内存中，则把内存中的<key, value>对排序然后写到磁盘文件中。
最后把内存缓冲区中的数据排序之后和那些磁盘文件组成一个最小堆，每次从最小堆中读取最小的数据，这个和MapReduce中的merge过程类似。

注意:想不明白的问题，reduce总是要根据key相同的数据进行运算的,那么对key排序不是一个必然发生的事情么？何必不在map上进行排序呢？
难道是reduce阶段不需要得到key相同的集合，而是只要有key在一个节点就可以?那也不需要按照key shuffle了呀？好奇怪的逻辑。


