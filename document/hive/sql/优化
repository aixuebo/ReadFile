1.当结果集数据文件很多,但是每一个文件内容字节很少,则在shuffle阶段会耗费很多时间,因此设置reduce数量,可以优化
set mapred.reduce.tasks = 1;
demo:从15分钟的洗数据,最终结果改成3分钟

优化的原因；hive当输入源很大的时候,比如8G,会有200多个map,因此就会有200多个结果集,
因此他预估reduce也要很多,比如reduce预估100个,因此这100个都会去那200个map中获取信息,这样就会产生100*200个连接请求,
因此速度会慢很多了,而设置1个reduce后,就变成1*200,连接从2万变成200个,速度提升100倍。
