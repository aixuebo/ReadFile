1.查询一个表,执行多个insert插入
FROM nginx n
INSERT OVERWRITE TABLE shareStatis
 PARTITION (task = 'share', date = '20150905')
 SELECT substr(n.accessTime,0,2)  as hour,n.platformId,1,n.coohuaId,n.sharedTitleId,n.sharedTextId,n.age
 where n.date = 20150905 AND n.fileType = 'shareFile' AND n.requestType = 'HEAD' AND strContain(n.requestUri,'/share.txt?') ='1'
INSERT OVERWRITE TABLE shareStatis
 PARTITION (task = 'shareSuccess', date = '20150905')
 SELECT substr(n.accessTime,0,2)  as hour,n.platformId,1,n.coohuaId,n.sharedTitleId,n.sharedTextId,n.age
 where n.date = 20150905 AND n.fileType = 'shareFile' AND n.requestType = 'HEAD' AND strContain(n.requestUri,'/share_success.txt?')='1'
 
2.行转列
获取date、小时、userid下 不同表的数据汇总,比如该用户在该小时有多少笔投资和投资金额、赎回笔数、赎回金额
只要四个属性任意一个满足,则都把这行数据输出出来,其他字段默认是0即可
方案:
a.可以用union将每一个表的数据查询出来,并且多写一个标识符字段,表示数据出于什么表的结果，
b.对a的结果进行group by date、小时、userid,select中根据标识符进行case when then处理即可。

或者将所有的可能列情况都写在一行,只是没有值的时候用0表示即可,这样最终外层循环用sum就可以了


3.学习使用full join操作,相当于left join和right join的综合版
测试的话可以获取两个表的各两条记录,full join后还是4条记录
该方式也可以完成2行转列的操作

注意:
a.3个表以上的full join是有问题的,因为两个表full join后,主键位置就不固定了,因为会出现两个表的主键都在同一行,但是数据却有的是null。
因此三个表以上full join的时候有问题。
b. 注意该表用full join ,因此对于关联的主键要用COALESCE(money.userid,interest.userid)处理

4.设置动态分区
a.set hive.exec.dynamic.partition.mode=nostrick;
----------------------------------------
b.insert overwrite table dim_jlc.topic_user_partition partition(log_day)
c.select的最后几个字段的值就是分区的值

注意:动态分区不要过多,否则会引起问题,因此有时候太多分区的话,要分批进行,比如我这边时间跨度是1年半,我就按照季度执行一次即可。
当然你也可以写mr自己做这事儿,但是有时候不必要那么麻烦,多循环几次也还可以接受。

5.在select中用两个case when 的sum做差,获取连续两天的数据增量
sum(case when log_day = '20160623' then premium_final else 0 end )-sum(case when log_day = '20160622' then premium_final else 0 end ) premium_final_last,

6.not in或者in的sql转换成
left join,然后在最后用where条件,将column is null 或者将column is not null来决定是否是not in 还是in。
其中column is null 表示not in 

7.join on中不允许使用函数,比如 substr(a.open_time,1,10) = b.create_date
解决方法是将 substr(a.open_time,1,10)写在a表的select里面,单独使用一个别名代替,然后on的时候使用a.别名 = b.create_date

8.有一个时间表,用于与正常表进行join操作。
on的条件是>= <=都可以,可以解决我们129个字段的时候,没办法全量一次性跑完历史数据的需求


