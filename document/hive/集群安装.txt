一、下载包 并且解压缩
curl -O "url"
tar -xzvf /aixuebo/dev/apache-hive-1.2.1-bin.tar.gz
mv apache-hive-1.2.1-bin/ hive-1.2.1
配置环境变量
export HIVE_HOME=/aixuebo/dev/hive-1.2.1
export PATH=$HIVE_HOME/bin:$PATH
source /etc/profile
二、在hadoop上创建目录和权限
hadoop fs -mkdir /tmp
hadoop fs -mkdir /user/hive/warehouse
hadoop fs -chmod g+w /tmp
hadoop fs -chmod g+w /user/hive/warehouse

三、执行hive即可
启动hadoop  执行hive

测试

hadoop fs -mkdir -p /log/statistics/etl/history_date
hadoop fs -put aaa.txt /log/statistics/etl/history_date/

CREATE TABLE IF NOT EXISTS default.history_date (
hdate STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LOCATION '/log/statistics/etl/history_date';

-----------------------------
让hive连接到mysql数据库
要配置文件配置上mysql的连接串

以下是网友遇见的问题,暂时我还没有做确定是正确的

建议先建元数据库，设置编码latin1。否则建好元数据相关可能会出问题，如drop table 卡死， create table too long等等
hive对utf-8支持不好。设置完编码latin1，发现table 字段描述无法显示中文。修改元数据库表的字符
(1)修改表字段注解和表注解
alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8
alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8
(2) 修改分区字段注解：
alter table PARTITION_PARAMS  modify column PARAM_VALUE varchar(4000) character set utf8 ;
alter table PARTITION_KEYS  modify column PKEY_COMMENT varchar(4000) character set utf8;
(3)修改索引注解：
alter table INDEX_PARAMS  modify column PARAM_VALUE  varchar(4000) character set utf8;

连接元数据设置
dbc:mysql://192.168.209.1:3306/metastore_hive_db?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8

对于已经建好的表，不起作用。 最好安装的时候就修改编码格式。

元数据mysql远程模式配置
<property>
                <name>hive.metastore.uris</name>
                <value>thrift://192.168.223.129:9083</value>
                <description>运行hive的主机地址及端口（特别重要ip不要弄错）</description>
</property>

启动元数据
bin/hive --service metastore &


