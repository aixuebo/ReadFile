# 背景与总结


# 一、Embedding
Embedding是一种将原始数据转换为对机器学习模型更有用的向量数据的技术。通过Embedding，原始数据的复杂关系和特征可以被压缩并保留在一个更紧凑、更易于处理的向量空间中。
## 1.核心逻辑
利用算法模型，将高维转换成低维，将稀疏向量转换成密集向量的过程。
比如单词词典如果用one-hot编码，则每一个句子，都需要用所有单词对应的向量来代替，此时是稀疏的，因为句子中单词占词典的占比不高。
如果算法把任何长度的单词组成的句子，转换成固定元素的向量，比如500维向量，此时向量基本上是密集的，但同时又能表示句子中单词之间的关系。

比如每一个单词都转换成512的向量中。而句子之间的相似性，又是基于单词组成的大向量进一步神经网络等训练的结果。

## 2.特点
* 降维：从高维空间映射到低维空间。稀疏向量转换成密集向量。
* 学习:实现一种自学习的算法模型，可以发现训练数据集中数据的关系。低维中依然保留原始数据的某些重要特征和关系。
* 捕捉语义:相近的单词在Embedding中通常距离较近。
* 预训练的Embedding模型：比如常见的预训练模型Word2Vec、GloVe和BERT，
Embedding可以基于预训练的模型做微调(匹配自己的素材的Embedding，比如Word2Vec本身不支持抖音关键词，微调意味着加入抖音关键词)或者直接使用。

## 3.应用
* 相似性：通过距离度量算法，比如余弦相似度，来衡量向量之间的相似性。即比较两个Embedding过的向量相似度。
* 跨模态Embedding:用于音频、图像等非文本数据。比如通过卷积神经网络CNN，将图像Embedding成一个密集的向量，相似的图像就有了相似性，用于做图像分类或者检索。
* 上下文感知:基础的单词Embedding，比如Word2Vec，是为每一个单词分配固定的向量，不考虑上下文；上下文感知Embedding（如ELMo、GPT和BERT）能够根据单词在句子中的上下文动态生成Embedding。

## 4.举例
输入每一个水果，以及每一个水果的特征，将其转换成数字，比如数字越大表示甜度越高；颜色用枚举值，并且越相似的颜色枚举值约相似。
苹果：甜度8，颜色红色，大小中等。
香蕉：甜度9，颜色黄色，大小大。
柠檬：甜度2，颜色黄色，大小小。
这样就将水果进行向量编码，就可以识别水果之间的关系，如果香蕉和苹果都是甜的，柠檬是酸的。


# 二、Tensor（张量）和Embedding的关系和区别
## 1.Tensor
其实就是一个容器的概念，可以理解成向量。
比如:
0阶张量是一个数（标量）。
1阶张量是一个线性数组（向量）。
2阶张量是一个矩阵。
更高阶的张量可以理解为更高维度的数组。
## 2.Embedding是一种方法，如何将数据转换成向量的算法模型，但最终Embedding的结果就是一种Tensor张量、或者说是向量。
