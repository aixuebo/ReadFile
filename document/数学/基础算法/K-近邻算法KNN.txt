一、背景与总结




二、K-近邻算法（K-Nearest Neighbor，K-NN）是一种基本的分类与回归方法
1.定义与计算方式。
它的基本思想是通过测量不同数据点之间的距离来进行分类或预测。
在K最近邻算法中，K代表着选择最近邻居的数量。

对于给定的一个新数据点，算法会计算它与训练数据集中所有数据点之间的距离，并选择离它最近的K个邻居。
a.对于分类问题，它会根据这K个邻居中最常见的类别来预测新数据点的类别。
b.对于回归问题，它会根据这K个邻居的平均值或加权平均值来预测新数据点的输出值。

2.计算过程
a.标记样本集，输入：样本特征 以及 输出分类(用于分类)或者输出值(用于回归)。
对数据进行归一化处理，确保各维度的贡献度一样。
b.对于一个待预测的新数据点，计算与所有样本集中的点的距离，找到最近的K个样本。
c.对于最近的K个样本，进行分类与回归的预测输出。


三、优缺点
1.优点
原理简单，容易实现，对于边界不规则数据的分类效果好于线性分类器。

2.缺点
每一个节点都要与所有节点计算距离，如果节点多，并且维度多，则计算量大，不可行。


四、注意事项
1.为了确保各个特征对距离的贡献相等，通常会进行特征归一化处理。
归一化的目的是确保每一个维度都是对结果贡献相等，不能因为某一个维度是长度，可能取值范围偏大，不归一化则会影响权重。
2.在K最近邻算法中，K的选择对结果有重要影响。
较小的K值会导致模型更敏感，可能受到噪声的影响；
而较大的K值会平滑决策边界，可能忽略了类别之间的细节差异。
3.K的选择方式
a.经验法：根据经验或领域知识选择一个常用的K值。例如，对于常见的分类问题，通常选择K=3或K=5。
b.交叉验证：使用交叉验证技术（如K折交叉验证）来评估不同K值下的模型性能，选择在验证集上表现最好的K值。
