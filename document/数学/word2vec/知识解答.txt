word2vec是将一个单词转换成固定长度的向量。那么会产生以下一些问题:
1.词可以描述成相似的，但句子如何描述相似性呢?
简单的方式是将句子拆分成词，然后每一个词的向量进行均值或者求和，即将句子转换成向量，然后比较句子向量之间的相似性。
更高级的方式,由于句子中相同的词但重要性不同，因此可以优化，加权求平均，至于加权如何做?可以是bert神经网络，也可以简单的id-idf词频统计算词的权重。

2.句子如何拆分成词?因为词才是word2vec需要的。
以下几种方式:char、bigram、truple、分词.
分词的缺点是 需要自己构建词表，但词表不会那么准，什么都包含,所以还是有问题的。
char表征的字是有限的,比如黄焖鸡、黄焖锅,这就有问题了。
bigram表征的字比char多,可以考虑到前后子的关系,因此更优。
truple表征的更多,理论上更优，但计算量也更大。
因此连续的词越多,表征的越多,计算量越大,现实中需要评估计算量大,带来多少的优化，最终发现优化其实挺小的,因此常用的是bigram和truple。

3.使用word2vec实现推荐物品，为什么呢?
常规理解是,每一个人买的物品顺序是不同的，听的歌曲顺序也是不同的，下载app的顺序也是不同的，按照时间顺序排序购买商品、看过的商品、听过的歌曲，为什么可以训练出歌曲的相似性、物品的相似性呢、app的相似性呢?
原因是他有一个前提假设,他找的相似性，是在找针对某一个场景下，搜索的结果相似性。
比如我要去旅游，我先下载携程、在下载去哪儿。。等等，这个肯定是有顺序的，这个时候根据同类app的数量，设置好合适的windowsize,就可以实现推荐的功能了。

更一步晋级，由于windowsize和同类型app数量是很难匹配好的，所以这种方式有弊端，因此bert也是可以将一个词转换成向量，但他更高级了。
相同的词，word2vec只能转换成相同的向量，但bert不会，他生成的向量,不仅和词有关系，还和词在上下文的位置有关系。因此bert产生的向量就可以表达出更丰富的内容。
