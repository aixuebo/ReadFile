参见 odsdata_etl_hadoop项目
InterestRecordSecondSortDriver

1.要有一个自定义对象,first seconde两个属性,并且按照first-seconde进行排序,map的key就是该对象
因此map过后可以保证到reduce中该对象first-seconde是有序的。
2.重新写一个PARTITION
job.setPartitionerClass(StringStringPartation.class);
因此只要保证该FIRST-seconde在同一个文件里面,即在同一个PARTITION里面就可以了，因此按照FIRST进行PARTITION就可以保证这点,因此重写PARTITION
3.重新写一个group
job.setGroupingComparatorClass(StringStringGroup.class);
因为虽然FIRST-SECOND都在同一个PARTITION中,并且按照顺序已经排列好了,但是没有办法让他们在同一个reduce方法里面，
该方法只让FIRST相同即可表示相同的FIRST就在同一个reduce中了



注意问题:
1.因为3,同一个reduce中的KEY就是最后一个FIRST-SECOND,因此SECOND要是有用的话,应该放到VALUE中
2.如果是想要倒序,则在自定义对象中,使用后者-前者就可以了,或者在前者-后者前面加一个负号
例如
    public int compareTo(StringStringCompare o) {
        int result = this.getFirstKey().compareTo(o.getFirstKey());
        return result!=0 ? result : -this.getSecondKey().compareTo(o.getSecondKey());
    }

-------总结 
核心是定义一个可二次排序的对象PairWritable。这样到reduce中他就是天然排序好的数据。
然后只需要reduce阶段可以识别哪些PairWritable是归属同一个key的即可。即job.setGroupingComparatorClass。
此时是没有性能消耗的，因为他是一行一行处理数据，所以天然依然是迭代器。

hadoop的二次排序
1.数据源是key,value。
2.创建一个对象1,包含key和value，整体作为key。并且支持排序。
3.同时创建一个partition对象，按照原始的key做为分区，确保原始的key传输到同一个reduce中。
4.由于reduce本身是支持key排序的，因此reduce内接收到的数据是对象1排序后的文件。
此时他们已经是完全经过二次排序的数据了。
但reduce内是按照对象1对象进行分组的，很显然这是不对的，我们要按照原始的key进行分组，所以要设置一个二次分组对象。
public class FirstGroupingComparator 
    implements RawComparator<PairWritable>{

    public int compare(PairWritable o1, PairWritable o2) {
        return Integer.valueOf(o1.getFirst()).compareTo(o2.getFirst());
    }

    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
        return WritableComparator.compareBytes(b1, 0, l1 - 4, b2, 0, l2 - 4);
    }

}
即reduce中接受到的PairWritable对象，如何进行分组


job.setMapOutputKeyClass(PairWritable.class);
job.setMapOutputValueClass(IntWritable.class);
job.setMapperClass(SecondarySortMapper.class);


job.setPartitionerClass(FirstPartitioner.class);## 何如按照key分组
job.setGroupingComparatorClass(FirstGroupingComparator.class);##reduce节点如何二次分组

job.setOutputKeyClass(IntWritable.class);
job.setOutputValueClass(IntWritable.class);
job.setReducerClass(SecondarySortReducer.class);