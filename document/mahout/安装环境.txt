mahout
1.http://apache.fayea.com/mahout/0.12.2/ 下载200多M的文件
2.解压缩 unzip source -d target
3.配置环境变量
创建s文件
export MAHOUT_HOME=/server/app/mahout/apache-mahout-distribution-0.12.2/apache-mahout-distribution-0.12.2
export JAVA_HOME=/server/java/jdk1.8.0_60
export HADOOP_HOME=${HADOOP_HOME:-/usr/hdp/current/hadoop-client}
export HADOOP_CONF_DIR=${HADOOP_HOME}/conf
export PATH=$MAHOUT_HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$HADOOP_HOME:$HADOOP_HOME/lib:$MAHOUT_HOME:$MAHOUT_HOME/lib:$CLASSPATH

source s
4.执行 bin/mahout 输出一堆算法列表,则说明完成安装
5.测试在hadoop上能否执行
找一个text文本的输出,将其转换成SequenceFile文件
sh ./bin/mahout  seqdirectory --input path --output path
6.测试canopy算法
a.创建输入路径和输出路径,
b.上传输入数据
input.log文件
7 7
8 8
6 6
9 9
2 2
1 1
0 0
2.9 2.9
c.执行命令
sh bin/mahout org.apache.mahout.clustering.conversion.InputDriver --input /log/mahout/canopy_test --output /log/mahout/output/canopy_test
-v org.apache.mahout.math.RandomAccessSparseVector
其中
-v org.apache.mahout.math.RandomAccessSparseVector是默认值,
将text转换成SequenceFileOutputFormat,输出的SequenceFileOutputFormat的key是text类型,值是VectorWritable的维数,value是VectorWritable对象,
VectorWritable对象来自于一行文本,按照空格进行拆分的集合


d.canopy算法要求输入源是SequenceFileOutputFormat类型文件,输入参数key-value分别是Text和VectorWritable
sh bin/mahout canopy --input  hdfs://namenode01.xxx.com:port/log/mahout/output/canopy_test --output  hdfs://namenode01.xxx.com:port/log/mahout/output/canopy_test1
--distanceMeasure org.apache.mahout.common.distance.EuclideanDistanceMeasure --t1 3 --t2 2 --overwrite  --clustering

程序执行也可以
hadoop jar /server/app/test/job/hadoop_temple.jar org.apache.mahout.clustering.canopy.CanopyDriver -libjars $test $* 1>>$logPath/appout.log 2>>$logPath/apperr.log
sh mathout.run.sh --input hdfs://namenode01.xxx.com:port/log/mahout/output/canopy_test --output hdfs://namenode01.xxxx.com:port/log/mahout/output/canopy_test1 --distanceMeasure org.apache.mahout.common.distance.EuclideanDistanceMeasure --t1 3 --t2 2  --overwrite --clustering

8.查看帮助命令
 sh bin/mahout canopy --help

9.查看mahout的输出
sh bin/mahout vectordump -i hdfs://path:port/log/mahout/output/canopy_test1/clusteredPoints

10.kmeans
方法1
sh bin/mahout kmeans \
-i path:port/log/mahout/output/canopy_test \
-o path:port/log/mahout/output/canopy_test_kmeans2 \
--clusters path:port/log/mahout/output/canopy_test_kmeans_center2 \
-k 2 \
-dm org.apache.mahout.common.distance.EuclideanDistanceMeasure \
-x 3 -ow -xm mapreduce

方法2
hadoop jar /server/app/test/job/hadoop_temple.jar org.apache.mahout.clustering.kmeans.KMeansDriver -libjars $test $* 1>>$logPath/appout.log 2>>$logPath/apperr.log

sh mathout.run.sh -k 2 --randomSeed 50 \
--input path:port/log/mahout/output/canopy_test \
--clusters path:port/log/mahout/output/canopy_test_kmeans_center \
--output path:port/log/mahout/output/canopy_test_kmeans1 \
--distanceMeasure org.apache.mahout.common.distance.EuclideanDistanceMeasure \
--maxIter 3 --overwrite --clustering

注意事项:
1.计算kmeans的时候,一个大的有顺序的文件,会有更好的聚类效果
