一、背景与总结
1.使用场景
日志系统、监控系统场景，特点是数据量存储非常大，写入频繁，但检索时候只关注最近一段时间内的数据，而不是全范围查询。
频繁写入B+树，会使节点不断分裂，（因为节点内索引是要排序的）。
同时每次插入的新数据都需要随机写入叶子节点的磁盘，而随机写入的性能非常慢。
如果是一个日志系统，每秒钟要写入上千条甚至上万条数据，这样的磁盘操作代价会使得系统性能急剧下降，甚至无法使用。

二、LSM树（Log Structured Merge Trees）
1.如何利用批量写入代替多次随机写入？
磁盘随机写入性能差，但顺序写入性能高，我们能否一次插入一个数据块，而不是一条数据?

数据要写入磁盘时，延迟写入，先在内存中维护，内存也是树结构存储。
当内存中的树持续变大达到阈值时，再批量地以块为单位写入磁盘的树中。当写满了一棵小树后，将小树与大树做merge。效率自然就高了。
查询的时候，小树、大树一起查，如果小树有查询结果，覆盖大树即可。

因此LSM树至少需要由两棵树组成，一棵是存储在内存中较小的C0树，另一棵是存储在磁盘中较大的C1树。

2.带来的问题
a.C1存储在磁盘上，可以使用B+树结构存储。
b.C0树在内存里，本身随机访问效率高，因此没必要使用B+树，而使用二叉树即可，同时每一个节点不需要存储索引，而是存储所有数据即可。
c.如何保证批量写之前系统崩溃可以恢复？
如果机器断电或系统崩溃了，那内存中还未写入磁盘的数据岂不就永远丢失了？这种情况我们该如何解决呢？
使用WAL技术（Write Ahead Log，预写日志技术）将数据第一时间高效写入磁盘进行备份。
日志是顺序写入磁盘的，写入效率高。日志写入磁盘后，在写入内存树。
系统会周期性地检查内存中的数据是否都被处理完了（比如，被删除或者写入磁盘），并且生成对应的检查点（Check Point）记录在磁盘中。然后，我们就可以随时删除被处理完的数据了。
	系统这样一来，log文件就不会无限增长了。
系统崩溃重启，我们只需要从磁盘中读取检查点，就能知道最后一次成功处理的数据在log文件中的位置。接下来，我们就可以把这个位置之后未被处理的数据，从log文件中读出，然后重新加载到内存中。

三、如何将内存数据与磁盘数据合并？
1.滚动合并（Rolling Merge）
我们可以参考两个有序链表归并排序的过程，将C0树和C1树的所有叶子节点中存储的数据，看作是两个有序链表，那滚动合并问题就变成了我们熟悉的两个有序链表的归并问题。

2.步骤
第一步，以多页块为单位，将C1树的当前叶子节点从前往后读入内存。读入内存的多页块，叫作清空块（Emptying Block），意思是处理完以后会被清空。
第二步，将C0树的叶子节点和清空块中的数据进行归并排序，把归并的结果写入内存的一个新块中，叫作填充块（Filling Block）。
第三步，如果填充块写满了，我们就要将填充块作为新的叶节点集合顺序写入磁盘。这个时候，如果C0树的叶子节点和清空块都没有遍历完，我们就继续遍历归并，将数据写入新的填充块。
如果清空块遍历完了，我们就去C1树中顺序读取新的多页块，加载到清空块中。
第四步，重复第三步，直到遍历完C0树和C1树的所有叶子节点，并将所有的归并结果写入到磁盘。这个时候，我们就可以同时删除C0树和C1树中被处理过的叶子节点。这样就完成了滚动归并的过程。

3.总结
限制内存一定大小后，顺序的读取两棵树的数据块，读取的内容填充到内存中。
因为磁盘的顺序读写性能和内存是一个数量级的，读取磁盘性能高。
merge在内存操作，性能也高。

四、LSM树是如何检索的？
1.先查询内存树，如果查询到，则直接返回，不需要查询C1树。
2.如果内存查询不到，则查询C1树。
如果一个系统的检索主要是针对近期数据的，那么大部分数据我们都能在内存中查到，检索效率就会非常高。


注意:删除操作
如果删除了，则也会在内存树插入该数据，并且标注好是删除的数据。
因此删除的数据，也会在内存树被查询到。
在滚动归并的时候，我们会查看数据在C0树中是否带有删除标志。如果有，滚动归并时就将它放弃。这样C1树就能批量完成“数据删除”的动作。


