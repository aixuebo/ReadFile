一、背景与总结
1.场景
全文检索，基于内容或者属性的检索场景，我们可以使用倒排索引完成高效的检索。
因为全文检索，哪个字段都可能是索引，因此B+树就不合适了。而应该使用倒排索引。

2.基于map-reduce的方式。
map端:将每一个节点排序成词与元组的映射: term -->  [<docId、pos位置、tf...>, <docId、pos位置、tf...>]
reduce端,基于相同的term,做merge元组。
同时产生reduce字典表。

3.如果字典表很大，我们可以使用B+树索引结构，存储字典表。
即相当于字典表用mysql这样的B+树存储，B+树叶子节点就是posting list磁盘的位置。
从而读取到磁盘上对应的倒排索引信息。

二、如何生成大于内存容量的倒排索引？
1.小规模时，内存中的倒排索引如何生产的
a.给每个文档编号，作为它们的唯一标识，并且排好序；
b.按照顺序扫描每一个文件，解析关键词后，创建一个数据对key --> <关键字，文档ID，关键字位置>,
c.按照key排序，然后按照文档id排序，存储到内存中。

2.工业级别的倒排索引，更复杂
a.有一个词典，将词编号与词字符串编码,而不是存储关键词字符串
比如说，如果文档中出现了“极客时间”四个字，那除了这四个字本身可能被作为关键词加入词典以外，“极客”和“时间”还有“极客时间”这三个词也可能会被加入词典。
因此，完整的词典中词的数量会非常大，可能会达到几百万甚至是几千万的级别。并且，每个词因为长度不一样，所占据的存储空间也会不同。

b.在posting list中，除了记录文档ID，我们还会记录该词在该文档中出现的每个位置、出现次数等信息。

c.数据结构
字典表dictionary:  dic_id、String
posting list: <docId、pos位置、tf...>, <docId、pos位置、tf...> 多元组。
因此最终映射就是dic_id --> [<docId、pos位置、tf...>, <docId、pos位置、tf...>]

3.如何生产大规模倒排索引 --- map-reduce方式
第一，我们可以将大规模文档均匀划分为多个小的文档集合，并按照之前的方法，为每个小的文档集合在内存中生成倒排索引。
第二，内存数据输出到磁盘，每一个磁盘文件有序，按照key排序，相同key的按照doc排序。
注意:此时的key是本地节点的词，不是全局的词，因此不能保证全局唯一ID，因此存储的还是词字符串。
第三,多路归并排序的方式，合并有序的临时小文件。
第四,多路归并后,数据是完整的，因此每一个关键词key是可以有唯一编号的。字段创建完成。

三、如何使用磁盘上的倒排文件进行检索？
1.大倒排索引文件如何检索
我们知道倒排索引由term字典 + posting list倒排索引组成。
a.字典表理论上不会太大，可以放到内存里。
因此内存存储的是字典表term key ---> posting list在磁盘的位置 映射关系。
查询的时候，通过字典，查询到posting list磁盘位置，读取磁盘信息。

b.如果term字典表很大,不能加载到内存怎么办?
将词典看作一个有序的key的序列，那这个场景是不是就变得很熟悉了？是的，我们完全可以用B+树来完成词典的检索。
即相当于字典表用mysql这样的B+树存储，B+树叶子节点就是posting list磁盘的位置。

c.如果term词对应的posting list非常长，它是很有可能无法加载到内存中进行处理的，怎么办？
比如说，在搜索引擎中，一些热门的关键词可能会出现在上亿个页面中。

其实，这个问题在本质上和词典无法加载到内存中是一样的。posting list中的数据也是有序的。
因此，我们完全可以对长度过大的posting list也进行类似B+树的索引，只读取有用的数据块到内存中，从而降低磁盘访问次数。
因此将两个term公共的wordID提取出来，变成了在两个B+树归并排序，获取交集的逻辑。

包括在Lucene中，也是使用类似的思想，用分层跳表来实现posting list，从而能将posting list分层加载到内存中。而对于长度不大的posting list，我们仍然可以直接加载到内存中。

此外，如果内存空间足够大，我们还能使用缓存技术，比如LRU缓存，它会将频繁使用的posting list长期保存在内存中。
这样一来，当需要频繁使用该posting list的时候，我们可以直接从内存中获取，而不需要重复读取磁盘，也就减少了磁盘IO，从而提升了系统的检索效率。

