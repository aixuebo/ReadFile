一、背景与总结
1.主流的KV存储，架构原理差不多。
比如redis基于内存、LevelDB基于磁盘查询。
hbase、flink底层用的rockDB，hbase等，都是基于LSM演化而来。所以学习这套KV架构理论是在阅读代码和使用工具时，掌握原理是有价值的。

二、LevelDB背景材料
1.Google官方公布的LevelDb性能非常突出，官方网站报道其随机写性能达到40万条记录每秒，而随机读性能达到6万条记录每秒。
LevelDB主要突出的是写性能高，即面对的是写多读少的场景，同时在不善于随机读的情况下，还能达到6万条记录/秒，可见非常好的存储工具。
LevelDB写性能高，原因是基于LSM树优化而来的存储系统。

2.LevelDb特点 -- 高效读取磁盘数据，而不是吃内存。
LevelDb不会像Redis一样狂吃内存，而是将大部分数据存储到磁盘上。

3.LevelDb的写操作要大大快于读操作。为什么要这么设计一个写操作数据性能高的数据库呢？数据库不应该面对的事读性能高吗？
LevelDB的设计很面向实际工程, 它的设计方案来源于这几个实际需求:
第一，对写入性能要求高, 而读性能是次要的. 可能很多人对此表示不理解, 认为现实环境中读的次数远多于写的次数. 这是他们对LevelDB所在的层次有所误解. 
LevelDB是个最底层的存储库, 而大多数读数据应该在上层的cache中获取, 不应该过于依赖LevelDB本身的读操作(虽然其本身有一定程度的cache支持), 
另外对于查找不存在的记录确实每次都会访问到LevelDB, 不过通过适当配置的bloom filter可以避免绝大部分磁盘读操作.
而写操作就不一样了, 上层的写操作要频繁地落实到LevelDB上。 因此可以看出, 在LevelDB这个层次来看, 正常用法是写操作多于读操作的.
第二，数据库虽庞大, 但根据80/20定律, 只有少部分数据会频繁读写. 
LevelDB的分层就是解决此问题而设计的, 冷热数据很容易分到不同的层次, 位于低层的少部分热数据文件的访问会快很多, 位于高层的大量冷数据大部分时间都在磁盘里静静地躺着, 顺带增量备份也容易实现.
第三，数据写入经常是间歇突发的. 数据库的访问经常是不稳定的, 为了削峰填谷, LevelDB在写入时只做了最有必要的操作就返回了成功, 
即顺序附加到log文件并更新内存中的memtable. 当然数据库仅靠这样的操作无法高效访问数据, 所以利用没有数据访问期间的空闲时间, 
用后台线程慢慢整理数据. 既让每次操作看起来很快, 也能充分利用空闲的CPU和IO改进整体性能, 符合多核CPU时代的设计理念. 当然整理也分优先级的, 这也是分多层设计的原因之一.
第四，考虑到非关系型数据库的检索，往往都是针对近期的数据进行的，而近期数据存储在内存或者level较低的层磁盘里，可以提高检索效率。
应对这种特殊场景，即只查询近期的数据感兴趣的场景，使用B+树显然是不太划算的。


三、LevelDb架构
1.WAL: Write-Ahead Logging预写日志系统，在写入Memtable之前有一个WAL操作，确保数据的不丢。
因此一次写操作，涉及到磁盘的顺序写 + 内存写。所以LevelDb写入性能高。
2.内存中的MemTable -- 参与读写
3.内存中Immutable MemTable -- 只读操作，用于序列化到磁盘上，生成SSTable文件。
4.SSTable文件，按照key排序好的Immutable MemTable文件，序列化到磁盘上。
5.Manifest文件 -- 记录所有的SSTable对应的key范围以及层级元数据信息。
层级、SSTable文件名，文件最小的key、最大的key。
6.Current文件
记载当前的manifest文件名。
因为在LevleDb的运行过程中，随着Compaction的进行，SSTable文件会发生变化，会有新的文件产生，老的文件被废弃，Manifest也会跟着反映这种变化，
此时往往会新生成Manifest文件来记载这种变化，而Current则用来指出哪个Manifest文件才是我们关心的那个Manifest文件。


四、compaction机制
1.levelDb的compaction机制和过程与Bigtable所讲述的是基本一致的，Bigtable中讲到三种类型的compaction: minor ，major和full。
所谓minor Compaction，就是把memtable中的数据导出到SSTable文件中；
major compaction就是合并不同层级的SSTable文件，
而full compaction就是将所有SSTable进行合并。

LevelDb包含其中两种，minor和major。




