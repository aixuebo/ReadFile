一、背景与总结
参考"图最近邻检索"

1.局部敏感哈希更擅长处理字面上的相似而不是语义上的相似
即从文章内容上相似，可以使用局部敏感哈希，但如果图片内容是否相似，音乐内容是否相似，这非文字识别，而是语义识别，并非局部敏感哈希擅长的。
相同的图片，这种语义识别相似性检索，如何实现？是本文要讨论的问题。

2.应用场景
以图搜图、拍图识物。这些都是图片检索领域的内容。

3.算法实现原理
a.使用神经网络，提取图片特征，将图片像素点组成的大向量 ---> 压缩成1024维小向量。
b.使用聚类算法，对向量划分区域。因为SimHash不能识别语义粒度的划分，而图片角度不同，提取的向量特征差异很大，因此属于语义识别领域问题，不能用SimHash算法计算相似性。

二、计算机如何使用向量表达一个图片呢?
1.简单暴力的方式，每一个像素点组成的向量即可表示图片。向量中的值是像素点的RGB值。
那一张图片的维度会是百万级别的。这么高的维度，检索起来会非常复杂，我们该怎么处理呢？
我们可以像提取文章关键词一样，对图片进行特征提取来压缩维度。

2.图片特征提取
有很多种深度学习的方法可以选择，比如，使用卷积神经网络（CNN）提取图片特征。这样，用一个512到1024维度的向量空间模型，我们就可以很好地描述图像了。

3.使用SimHash计算的相似度结果可能就会效果非常差。
局部敏感哈希由于哈希函数构造相对比较简单，往往更适合计算字面上的相似性（表面特征的相似性），而不是语义上的相似性（本质上的相似性）。
举个例子，即便是面对同一种花，不同的人在不同的地点拍出来的照片，在角度、背景、花的形状上也会有比较大的差异。也就是说，这两张图片的表面特征其实差异很大，
这让我们没办法利用局部敏感哈希，来合理评估它们的相似度。

这样角度不同，造成的图片特征不一样的情况下。使用SimHash计算的相似度结果可能就会效果非常差。

4.K-Means聚类的方式，更适合计算相似性区域划分。

三、如何使用聚类算法进行相似检索？
1.检索常规方案
既然已经使用K-Means聚类图片特征信息，那么如何快速检索呢，逻辑步骤也是与前面方案大体一致的。
a.因为图片特征压缩后有1024维，所以聚类中心点也设置1024个。(多少都可以自己定)。
每个中心点，进行索引，key是中心点向量，value是该中心点下所有的图片向量集合。
b.当要查询一个点邻近的点时，先计算该点与所有中心点的距离，获取最近的中心点。
c.查询最近中心点索引信息，获取所有图片向量集合。完成粗筛选。
d.两两计算，该点与同一个中心点的图片向量集合中的点，获取topK个相似的图片。

2.可能存在的优化的空间
最近的聚类中的节点数非常多。这个时候，我们就计算该聚类中的所有节点和查询点的距离，这个代价会很大。该如何优化？
假设有1万个聚类中心点，每次比较1万个，代价太大了。
因此分层，以参考二分查找算法不断划分子空间划分的思路，使用层次聚类将一个聚类中的节点，再次划分成多个聚类。
比如一级中心点，二级中心点，三级中心点。


四、如何使用乘积量化压缩向量？ ---- 以下内容有点抽象，理解不是很到位 ，可以直接参考五部分例子。
1.问题 --- 存储1亿个图片，需要300G，内存无法存放，所以不能再内存中完成检索
对于向量的相似检索，除了检索算法本身以外，如何优化存储空间也是我们必须要关注的一个技术问题。
以1024维的向量为例，因为每个向量维度值是一个浮点数（一个浮点数有4个字节），所以一个向量就有4K个字节。
如果是上亿级别的数据，光是存储向量就需要300G的内存，这会导致向量检索难以在内存中完成检索。

2.为了能更好地将向量加载到内存中，我们需要压缩向量。
即一个1024维度的向量，如何转换成更少的维度去参与比较？
想要压缩向量，我们往往会使用向量量化（Vector Quantization）技术。其中，我们最常用的是乘积量化（Product Quantization）技术。

a.量化指的就是将一个空间划分为多个区域，然后为每个区域编码标识。
比如说，一个二维空间可以被划为两块，那我们只需要1个比特位就能分别为这两个区域编码了，它们的空间编码分别是0和1。
那对二维空间中的任意一个点来说，它要么属于区域0，要么属于区域1。

这样，我们就可以用1个比特位的0或1编码，来代替任意一个点的二维空间坐标了 。
假设x和y是两个浮点数，各4个字节，那它们一共是8个字节。如果我们将8个字节的坐标用1个比特位来表示，
就能达到压缩存储空间的目的了。前面我们说的用聚类ID代替具体的向量来进行压缩，也是同样的原理。

而乘积指的是高维空间可以看作是由多个低维空间相乘得到的。我们还是以二维空间为例，它就是由两个一维空间和相乘得到。
类似的还有，三维空间是由一个二维空间和一个一维空间相乘得到，依此类推。

那将高维空间分解成多个低维空间的乘积有什么好处呢？它能降低数据的存储量。比如说，二维空间是由一维的x轴和y轴相乘得到。
x轴上有4个点x1到x4，y轴上有4个点y1到y4，这四个点的交叉乘积，会在二维空间形成16个点。
但是，如果我们仅存储一维空间中，x轴和y轴的各4个点，一共只需要存储8个一维的点，这会比存储16个二维的点更节省空间。

总结来说，对向量进行乘积量化，其实就是将向量的高维空间看成是多个子空间的乘积，然后针对每个子空间，再用聚类技术分成多个区域。最后，给每个区域生成一个唯一编码，也就是聚类ID。

3.举例
a.逻辑
一个样本 1024维度向量 ---> 分割成4份 --> 每一份256位的向量参与聚类，聚类结果也是256个聚类中心点。
假设有1万个样本，经过拆分变成4万个样本，对4万个样本，聚类算法，产生256个中心点，256个样本点使用8个bit编码，即一个字节即可编码序号，看每一个样本对应的样本序号。
因此一个样本 1024维度，每一个维度值是4个字节，占4k，变成4个聚类id，占用4个字节。 即优化从4k --> 4byte,就可以存放在内存里了。

b.步骤
假设样本向量都是1024维的浮点数。
按照256维做分割，因此可以分割成4份。(为什么256维分割，是因为2^8=256,即最后每一个分段，其实我们用8个bit位就可以代表)。
对每一个分段内的256维，每一个段向量做输入，进行聚类，聚类目标是划分为256个聚类。
每一个聚类中心点，我们编号，即1~256，用1个字节8位就可以表示。
因此最终得到1024个中心点，存储着1024个中心点。 每一个中心点占用空间 (256维度*4字节的浮点数维度值) = 1k。 1024个中心点一共1M。可以存储在内存。


具体逻辑:
如果我们的样本向量都是1024维的浮点数向量，那我们可以将它分为4段，这样每一段就都是一个256维的浮点向量。
然后，在每一段的256维的空间里，我们用聚类算法将这256维空间再划分为256个聚类。接着，我们可以用1至256作为ID，来为这256个聚类中心编号。
这样，我们就得到了256 * 4 共1024个聚类中心，每个聚类中心都是一个256维的浮点数向量（256 * 4字节 = 1024字节）。
最后，我们将这1024个聚类中心向量都存储下来。

这样，对于这个空间中的每个向量，我们就不需要再精确记录它在每一维上的权重了。
我们只需要将每个向量都分为四段，让每段子向量都根据聚类算法找到所属的聚类，然后用它所属聚类的ID来表示这段子向量就可以了。

因为聚类ID是从1到256的，所以我们只需要8个比特位就可以表示这个聚类ID了。
由于完整的样本向量有四段，因此我们用4个聚类ID就可以表示一个完整的样本向量了，也就一共只需要32个比特位。
因此，一个1024维的原始浮点数向量（共1024 * 4 字节）使用乘积量化压缩后，存储空间变为了32个比特位，空间使用只有原来的1/1024。
存储空间被大幅降低之后，所有的样本向量就有可能都被加载到内存中了。

五、如何计算查询向量和压缩样本向量的距离（相似性）？ --- 找到相似的图片，即以图搜图
参考"图最近邻检索"
1.解释一下 --- 即如何查询一个图片向量，与 压缩后4个字节的向量相似性比较？
因为输入的是图片，是非常大的向量，通过神经网络算法，转换成1024维向量。

2.这整个计算过程会涉及3个主要向量，分别是样本向量、查询向量以及聚类中心向量。
a.查询向量
将输入的图片 --> 神经网络算法转换成1024维特征向量 ---> 拆分成4段 
b.查询向量与聚类中心向量的距离
---> 计算每一段的聚类编号，即与内存中的该段的聚类中心几点计算，计算256次距离，看归属于哪个聚类点，从而获取4个段的聚类ID。
注意:中间有临时4个表，用于记录每一段与每一个中心点的距离。因此查询该表，就可以知道每一段的中心点归属了。也可以用于后续其他获取值的操作，避免再次计算。
c.计算查询向量 与 每一个样本向量的距离
样本向量也拆分成4段，分别计算每一个段的 查询向量与样本向量距离。
为了优化查询性能，我们已经将查询向量与样本向量子段都进行了聚类处理，所以计算距离时，我们直接使用聚类中心代替查询/样本向量。(因为聚类中心是在内存可以查询到的)
因此我们只需要查询b中缓存的数据表，可以知道查询向量子段的聚类ID与样本向量段的聚类ID的距离，记作D1。(查询效率极高O(1))。
注意:样本向量存储的是4个字节，每一个字节转换成十进制，就是聚类的ID。
---> 查询向量 与 样本向量 关于子段的距离 D1。
---> 查询向量 与 某一个样本向量，得到了4个距离D1。
---> 使用欧氏距离的方式,合并距离，得到 查询向量和样本向量的距离 = Math.开根号(D1^2+D2^2+D3^2+D4^2)。

3.结论
原本查询向量 与 样本向量，计算相似性是1024维度计算距离是复杂的。现在变得优化了。
a.样本向量每一个从1024维*4字节 = 4k，变成4个字节的4段聚类编号ID。可以存储在内存里了，不需要磁盘访问了。
b.四段256维的聚类中心点，一共1M的内存空间，也存储在内存里了。
c.查询向量，拆分成4段，不需要花费时间，计算好每一段与256*4=1024个聚类中心点的距离，在内存里就可以计算，很快。即1024次CU时钟。
并且结果存储在内存里。
d.查询向量与样本向量的距离，变成4次O(1)时间代价的查询C结果。然后做一个欧氏距离。

结果就是 复杂的计算，变成常数级的时间代价就可以计算完成。
