一、背景与总结
1.如何将最准确的结果返回给用户。即要解决如何对查询的结果进行打分的逻辑。
同时产生一个问题，一个term对应的倒排索引文章很多，此时如何在有限的时间内，检索到最优的topN结果，提出了挑战。


二、TF-IDF算法是什么 --- 它能很好地表示一个词在一个文档中的权重
1.TF-IDF算法的公式
相关性 = TF * IDF。这两个参数都是越大越好。
TF是词频（Term Frequency）= 一个词项在文档中出现的次数，如果一个词项出现了越多次，那这个词在文档中就越重要。
文档频率（Document Frequency）= 这个词项出现在了多少个文档中。如果一个词出现在越多的文档中，那这个词就越普遍，越没有区分度，比如"的"。
IDF是逆文档频率（Inverse Document Frequency）= 1 / 文档频率，逆文档频率是对文档频率取倒数，它的值越大，这个词的的区分度就越大。

不过，在计算的过程中，我们会对TF和IDF的值都使用对数函数进行平滑处理。
TF = 1 + log(tf)
IDF = log(N/(df+1)),N是所有文档的个数，df+1为了防止分母df为0的场景。即该词没有出现在该文档中。

2.如何用一个向量表示一个文章
a.文章由词组成。
b.我们可以使用TF-IDF算法，计算每一个词在文档中的权重。
c.因此文章等于全部词集合组成的向量，而向量的值就是该词在文章中的权重，即该词对于该文章的TF-IDF。

3.如何使用概率模型中的BM25算法进行打分？ ---- 他是TF-IDF算法的一种升级。
在实际使用中，我们往往不会直接使用TF-IDF来计算相关性，而是会以TF-IDF为基础，使用向量模型或者概率模型等更复杂的算法来打分。比如说，概率模型中的BM25（Best Matching 25）算法。

BM25算法设计思想：它认为词频和相关性的关系并不是线性的。也就是说，随着词频的增加，相关性的增加会越来越不明显，并且还会有一个阈值上限。当词频达到阈值以后，那相关性就不会再增长了。
因此BM25的打分公式: (k1+1)*tf / k1+tf。
其中，k1是可以人工调整的参数。k1越大，权重上限越大，收敛速度越慢，表示tf越重要。
在极端情况下，也就是当k1 = 0时，就表示tf不重要。
按照经验来说，我们会把k1设为1.2。

公式解释原理:在这个公式中，随着tf的值逐步变大，权重会趋向于k1 + 1这个固定的阈值上限（将公式的分子分母同时除以tf，就能看出这个上限）。

三、如何使用机器学习来进行打分？
随着搜索引擎的越来越重视搜索结果的排序和效果，我们需要考虑的因子也越来越多。比如用户的历史点击行为是否也是相关性的一个衡量指标？
在当前的主流搜索引擎中，用来打分的主要因子已经有几百种了。如果我们要将这么多的相关因子都考虑进来，再加入更多的参数，那BM25算法是无法满足我们的需求的。
这个时候，机器学习就可以派上用场了。
原理很简单，就是把不同的打分因子进行加权求和。比如说，有n个打分因子，分别为x1到xn，而每个因子都有不同的权重，我们记为w1到wn，那打分公式就是：
Score = w1 * x1 + w2 * x2 + w3 * x3 + …… + wn * xn
那你可能会问了，公式中的权重要如何确定呢？这就需要我们利用训练数据，让机器学习在离线阶段，自动学出最合适的权重。

四、对结果进行打分后，进行排序，选择topN即可返回给用户。
但倒排索引中全部的文章，都进行机器学习的打分，成本太高了。可能很难再有限的短时间内，返回给用户。如何解，下节课主要解决的问题。

