一、背景与总结
1.搜索引擎根据关键词，计算好结果的排名，返回给用户。但如果返回的信息大多都是相近/雷同的文章，用户体验是不够好的。
文章去重该如何做，即转换成 如何检索相似文章。
对相似文章去重，本质上就是把相似的文章都检索出来。

2.什么是局部敏感哈希

3.SimHash -- 局部敏感哈希的一种工程实现
通过使用SimHash函数和分段检索（抽屉原理），使得Google能在百亿级别的网页中快速完成过滤相似网页的功能，从而保证了搜索结果的质量。

4.局部敏感哈希更擅长处理字面上的相似而不是语义上的相似
即从文章内容上相似，可以使用局部敏感哈希，但如果图片内容是否相似，音乐内容是否相似，这非文字识别，而是语义识别，并非局部敏感哈希擅长的。

二、如何在向量空间中进行近邻检索？
1.向量空间模型 Vector Space Model，计算机用来识别一篇文章。
一篇文章 = 关键词组成的向量。而向量中的值是tf-idf值。
假设所有文章对应的关键词一共n个，则每一个文章都是n维向量，每一个向量维度值是该文章的。
即<01,w2,w3,……wn>,这样一来，每一个文档就都是n维向量空间中的一个点。比较相似性，就找到空间向量中相近的点(文章)。
比如文章1 的向量为 <0,0,0.5,0.8,0>

2.计算相似文章，转换成计算向量的相似度。
a.给定一个文章，可以知道他的向量。
b.获取该向量最相近的topN向量，这个与geohash检索附近人一样逻辑。
因此先粗查找topK，然后再K里面精准排序，寻找topN即可。

三、近似最近邻检索（Approximate Nearest Neighbor）


四、什么是局部敏感哈希
1.思路与geohash划分区域一样，我们也要为"向量空间模型"划分区域，并且编码。粗查找到相近的文章。
目标是相近的文章，编码是相近的。

2.局部敏感哈希 -- Locality-Sensitive Hashing
让相似的数据通过哈希计算后，生成的哈希值是相近的（甚至是相等的）的算法。

3.思想实验
a.在二维空间中，随意画一条线，将其一分为二。一面定义为1，一面定义为0。
b.在继续划分，这样就形成了N多个子区域。
c.推到结论
如果空间中两个点很近，则大概率这两个点会被划分到同一个区域。因此这两个点的编码值大概率是相同的。
反过来 如果两个编码相同，也说明两个点很近，而每一个文章其实就是一个点。

4.实现方法
a.随机生成n个超平面，每个超平面都将高维空间划分为两部分。规定好上面是1，下面是0。
b.每一个文章对所有平面进行划分，每次划分的结果拼接起来，就是文章向量的编码。
c.如果有两个点的哈希值是完全一样的，就说明它们被n个超平面都划分到了同一边，它们有很大的概率是相近的。
即使哈希值不完全一样，只要它们在n个比特位中有大部分是相同的，也能说明它们有很高的相近概率。

因此使用海明距离（Hamming Distance），计算两个文章hash值的bit位差异数，差异越小，说明越相近。
我们可以认为如果两个对象的哈希值的海明距离低于k，它们就是相近的。一般情况，我们认为k=2是合理的。即我们认为海明距离在2之内的哈希值都是相似的，那它们就是相似的。
举个例子，如果有两个哈希值，比特位分别为00000和10000。你可以看到，它们只有第一个比特位不一样，那它们的海明距离就是1。

5.缺陷与问题
在原来的空间中，不同维度本来是有着不同权重的，权重代表了不同关键词的重要性，是一个很重要的信息。
但是空间被n个超平面随机划分以后，权重信息在某种程度上就被丢弃了。即没有利用到权重信息。


五、SimHash是怎么构造的
1.背景
那为了保留维度上的权重，并且简化整个函数的生成过程，Google提出了一种简单有效的局部敏感哈希函数，叫作SimHash。
它其实是使用一个普通哈希函数代替了n次随机超平面划分，并且这个普通哈希函数的作用对象也不是文档，而是文档中的每一个关键词。
这样一来，我们就能在计算的时候保留下关键词的权重了。

2.一篇文章，计算SimHash的步骤
a.选择一个能将关键词映射到64位正整数的普通哈希函数。
b.使用该哈希函数给文档中的每个关键词生成一个64位的哈希值，并将该哈希值中的0修改为-1
比如说，关键词A的哈希值编码为<1,0,1,1,0>，那我们做完转换以后，编码就变成了<1,-1,1,1,-1>。
c.将关键词的编码乘上关键词自己的权重。
如果关键词编码为<1,-1,1,1,-1>，关键词的权重为2，最后我们得到的关键词编码就变成了<2,-2,2,2,-2>。
d.将所有关键词的编码按位相加，合成一个编码。
如果两个关键词的编码分别为<2,-2,2,2,-2>和<3,3,-3,3,3>，那它们相加以后就会得到<5,1,-1,5,1>。
e.将最终得到的编码中大于0的值变为1，小于等于0的变为0。
这样，编码<5,1,-1,5, 1>就会被转换为<1,1,0,1,1>。

3.方案优点
a.通过这样巧妙的构造，SimHash将每个关键词的权重保留并且叠加，一直留到最后，从而使得高权重的关键词的影响能被保留。
b.SimHash通过一个简单的普通哈希函数就能生成64位哈希值，这替代了随机划分64个超平面的复杂工作，也让整个函数的实现更简单。

六、如何对局部敏感哈希值进行相似检索？
1.结论是，通过给文章计算SimHash编码后，找到对应的同区域的文章集合，再进一步两两计算，找到最相近的文章。
如果两个文档的SimHash值的海明距离小于k，我们就认为它们是相似的，google建议k=3

2.如何提高检索效率呢
一个直观的想法是，我们可以针对每一个比特位做索引。
由于每个比特位只有0和1这2个值，一共有64个比特位，也就一共有2*64共128个不同的Key。
因此我们可以使用倒排索引，将所有的文档根据自己每个比特位的值，加入到对应的倒排索引的posting list中。
这样，当要查询和一个文档相似的其他文档的时候，我们只需要通过3步就可以实现了，具体的步骤如下：

计算出待查询文档的SimHash值；
以该SimHash值中每个比特位的值作为Key，去倒排索引中查询，将相同位置具有相同值的文档都召回；
合并这些文档，并一一判断它们和要查询的文档之间的海明距离是否在3之内，留下满足条件的。

我们发现，在这个过程中，只要有一个比特位的值相同，文档就会被召回。也就是说，这个方案和遍历所有文档相比，其实只能排除掉“比特位完全不同的文档”。
因此，这种方法的检索效率并不高。

3.Google利用抽屉原理，设计更高效的检索方法 --- 通过使用SimHash函数和分段检索（抽屉原理），使得Google能在百亿级别的网页中快速完成过滤相似网页的功能，从而保证了搜索结果的质量。

a.原理介绍
抽屉原理，简单来说，如果我们有3个苹果要放入4个抽屉，就至少有一个抽屉会是空的。
那应用到检索上，Google会将哈希值平均切为4段，如果两个哈希值的比特位差异不超过3个，那这三个差异的比特位最多出现在3个段中，也就是说至少有一个段的比特位是完全相同的！
因此，我们可以将前面的查询优化为“有一段比特位完全相同的文档会被召回”。

即将hash值拆分成4段，因为海明距离<=3是相似的，因此4段中至少有一段是完全一样的。否则4段都不同，那意味着k>3,肯定不相似。
所以hash拆分的段数 与 海明距离k是有关系的。如果海明距离定义是5，则hash拆分的段数就应该是6。

b.实现
因为每一个hash是64位bit，所以拆分4段后，每一段16bit。
将每一个hash值按照16bit拆分成4段，即存在4个key，将文章id分别存储到4个倒排索引中。


检索的时候，计算待查询的文章hash值，并且计算好4个段的key。
分别查询4次倒排索引，得到4组posting list。这4组，都是相近的文档集合，因为命中任意一段的文章集合，都说明至少有一段是相同的。
将返回的四个posting list合并，并一一判断它们的的海明距离是否在3之内。

