一、背景与总结
1.数据存储流程
内存: 创建MemTable ---> 写入数据 ---> 达到上限后 ---> 原子操作将其转换成 Immutable MemTable  && 创建新的MemTable接收数据 ---> Immutable MemTable参与归并排序。
磁盘: Immutable MemTable --> SSTable
2.使用场景
他是key-value存储引擎，即给定key立刻返回value。同时key支持很久以前的数据删除的能力。
如何做到很久以前的数据会被删除的呢?因为实现上,越久的数据没有发生变化，他存储的层级越深，因此直接把某一个时间段之后产生的层级都删除即可。
因为作为缓存数据库，如果长时间没有被人访问，一直处于最底层的数据，已经失去缓存的意义了，所以可以直接被删除。
3.分层原理
磁盘第0层，按照SSTable数量设置为上限。原因是该层的SSTable无序，所以不需要控制文件总大小，只需要控制Immutable MemTable --> SSTable的数量即可。
从第1层开始，不再按照SSTable数量，而是按照SSTable总大小设置上限，原因是该层的SSTable是有序的。
归并排序本身效率可以，因为毕竟每一个文件都有顺序，所以不需要太浪费内存，但参与归并文件的数量太大，还是会影响的。所以控制一层文件总大小的方式，控制文件数量。

每一层存储空间扩大10倍。
因此第1层10M，第二次100M，第三次1G，第四层10G，第五层100G，第六层1T

4.扫描原理
查询磁盘，根据SSTable排序，可以二分法找到该层key可能存在的SSTable文件，然后通过SSTable文件的布隆过滤器确定key是否存在。不存在继续查找下一层。
因此磁盘中虽然存储了很多SSTable，但其实可以把SSTable的filter block内容存储到内存，就可以避免扫描此磁盘了。

二、什么是levelDB，以及levelDB为什么性能高
1.levelDB的背景材料
参考基础篇

2.如何做到的性能优化
我们知道，LSM树会将索引分为内存和磁盘两部分，并在内存达到阈值时启动树合并。但是，这里面存在着大量的细节问题。
a.数据在内存中如何高效检索。
内存树不是使用B+树，因为对内存中索引的高效检索，我们可以用很多检索技术，如红黑树、跳表等，这些数据结构会比B+树更高效。
因此，LevelDB对于LSM树的第一个改进，就是使用跳表代替B+树来实现内存中的C0树。

b.内存数据要如何高效存储到磁盘，如何避免加锁的情况下，解决数据一致性问题
因为内存树与磁盘上的树要进行归并，创建新的B+树。
但如果内存中的数据一边被写入修改，一边被写入磁盘，我们在归并的时候就会遇到数据的一致性管理问题。一般来说，这种情况是需要进行“加锁”处理的，但“加锁”处理又会大幅度降低检索效率。

为此，LevelDB做了读写分离的设计。
它将内存中的数据分为两块，一块叫作MemTable，它是可读可写的。另一块叫作Immutable MemTable，它是只读的。这两块数据的数据结构完全一样，都是跳表。
当MemTable的存储数据达到上限时，我们直接将它切换为只读的Immutable MemTable，然后重新生成一个新的MemTable，来支持新数据的写入和查询。
这时，将内存索引存储到磁盘的问题，就变成了将Immutable MemTable写入磁盘的问题。而且，由于Immutable MemTable是只读的，因此，它不需要加锁就可以高效地写入磁盘中。

即创建MemTable ---> 写入数据 ---> 达到上限后 ---> 原子操作将其转换成 Immutable MemTable  && 创建新的MemTable接收数据 ---> Immutable MemTable参与归并排序。

c.如何在磁盘中对数据进行组织管理 --- 磁盘上存储的是若干个SSTable对象，而不是整颗B+树。
在原始LSM树的设计中，内存索引写入磁盘时是直接和磁盘中的C1树进行归并的,但会有两个很严重的问题：
第一，合并代价很高，因为C1树很大，而C0树很小，这会导致它们在合并时产生大量的磁盘IO；
第二，合并频率会很频繁，由于C0树很小，很容易被写满，因此系统会频繁进行C0树和C1树的合并，这样频繁合并会带来的大量磁盘IO，这更是系统无法承受的。

因此LevelDB采用了延迟合并的设计来优化。
第一，先将Immutable MemTable顺序快速写入磁盘，直接变成一个个SSTable（Sorted String Table）文件。
第二，再对这些SSTable文件进行合并。
这样就避免了直接的C0树和C1树昂贵的合并代价

即Immutable MemTable --> SSTable,暂时不参与归并排序。

d.数据是如何从磁盘中高效地检索出来的？
先在内存MemTable中查找，然后再到内存Immutable MemTable查找，然后再到磁盘上SSTable中查找。

三、SSTable的分层管理设计
因为磁盘中原有的C1树被多个较小的SSTable文件代替了。那现在我们要解决的问题就变成了，如何快速提高磁盘中多个SSTable文件的检索效率。
即在整颗B+树查询 --> 变成了 如何在多个SSTable文件中快速定位数据的能力。

1.磁盘查询的背景知识
a.单独的SSTable中key是有顺序存储的，但磁盘上多个SSTable中的key是有重叠的，不知道某一个key到底在哪个SSTable里存在。所以需要查询每一个SSTable去查找key，因此会带来大量的磁盘随机访问开销。
比如说，第一个SSTable中的数据从1到1000，第二个SSTable中的数据从500到1500。那么当我们要查询600这个数据时，只能扫描全部的SSTable。
b.解决方案也很明显，就是将所有的SSTable中数据，按照key重新排序。因此可以使用二分法找到对应的key所在的SSTable，直接读取即可。
但是其实所有的SSTable保证有顺序，是有难度的。

比如开始的时候，有一个SSTable，是有顺序的；
新增一个SSTable的时候，两个文件就要做归并排序，创建新的SSTable。
而LevelDB为了查询性能，限制了每一个SSTable不要太大，最好不超过2M，因此意味着新增一个SSTable，就要和所有的SSTable做归并，并且生产出N多个SSTable。
随着SSTable文件的增多，假设有10万个SSTable，因此没新增一个SSTable，极端情况都要和10万个SSTable做归并，并且创建新的10万个SSTable，系统的读写性能都会受到很严重的影响。

这种多个SSTable进行多路归并，生成新的多个SSTable的过程，也叫作Compaction。

2.怎么降低多路归并涉及的SSTable个数呢 --- 滚动合并方法。
将SSTable进行分层管理。第0层控制SSTable数量，第1层之后，控制每一层文件大小，避免文件太大，造成归并排序压力大。
归并排序本身效率可以，因为毕竟每一个文件都有顺序，所以不需要太浪费内存，但参与归并文件的数量太大，还是会影响的。所以控制一层文件总大小的方式，控制文件数量。

a.Level 0层，只允许放4个SSTable。---- 该层没有参与归并，因此查询时，扫描全部4个SSTable文件进行查询。
从Immutable MemTable转成的SSTable，直接存储在第0层。即第0层是最新的缓存数据。
b.Level 1层 --- 有顺序的SSTable ---- 针对该层设置文件总量上限。默认10M
当第0层满了，我们对第0层4个SSTable文件进行归并，创建出新的4个排序好的SSTable。存放到第一层
当Level 0层满了以后，我们就要将它们进行多路归并，生成新的有序的多个SSTable文件，这一层有序的SSTable文件就是Level 1 层。
每次第0层满了，都会和level1去归并排序，创建新的有序的SSTable文件。
当第1层满10M时，归并到第2层。

c.当Level 1层的SSTable文件总容量达到了上限之后，我们就需要选择一个SSTable的文件。
注意一个优化细节，因为第1层假设有100个SSTable，他们都已经排序好了，但因为达到上限了，所以产生了第2层，虽然第2层比第1层可以容纳更多数据。
但没有必要一下全把第一层数据扔给第二层，清空第一层内容？其实这个方案也是可行的，但具体实现上，他只是随机拿第一层的某一个SSTable文件，移动到下一层而已。为第一层留下空间。
其实我是有保留意见的，为什么不全部扔到下一层？

我猜想原因是效率问题，因为上一层轮训的方式找到一个SSTable，而该SSTable是有key范围的，只需要将该SSTable与下一层所有的SSTable中，命中key范围重叠的下一层SSTable做并归即可。
减少并归的耗时。而所有上一层数据与下一层数据并归，效率还是比较慢的。

将它并入下一层（为保证一层中每个SSTable文件都有机会并入下一层，我们选择SSTable文件的逻辑是轮流选择。也就是说第一次我们选择了文件A，下一次就选择文件A后的一个文件）。

下一层会将容量上限翻10倍，这样就能容纳更多的SSTable了。依此类推。

四、如何查找对应的SSTable文件
1.先在第0层全部的SSTable中，查找是否有该key。有则返回，无则到第下一层查找。
2.从第一层开始，所有的SSTable中都已经按照key排序了，并且磁盘上有记录每一个SSTable的key范围，所以直接读取索引，二分查找的方式即可判断该层是否有key，以及key在哪个SSTable。
查找到则返回，无则到第下一层查找。一直到所有的层都查找完，发现还没有，则直接返回无。

五、如何加速SSTable文件中的检索--找到key对应的SSTable文件后，怎么检索key的内容呢？
SSTable文件格式:数据存储区 + 数据索引区。
	数据存储区存储数据块Data block，即存储key=value具体内容。
	数据索引区由末尾到头，分别存储:
		foot block,记录index block和meta index block的offset和size，方便快速加载两部分到内存。
		index block,数据索引，记录每一个key的位置，key+offset+size信息，即每一个key+value转换成key+8字节，即vakue转换成8个字节。
		meta index block,记录布隆过滤器的索引位置
		filter block，记录布隆过滤器信息，快速检索key是否存在。

因此结论是，查询磁盘，根据SSTable排序，可以二分法找到该层key可能存在的SSTable文件，然后通过SSTable文件的布隆过滤器确定key是否存在。不存在继续查找下一层。
因此磁盘中虽然存储了很多SSTable，但其实可以把SSTable的filter block内容存储到内存，就可以避免扫描此磁盘了。

六、SSTable数据文件的删除
因为层级越高，说明越长时间没有被访问了，如果访问的key，一定在低层级也会存在该key，所以高层级的key也就失效无用了。
因此设定一定周期的数据就丢弃，可以转换成其实高层级的数据基本上都超过了日期周期范围，直接物理删除文件即可。


