一、背景与总结
1.流处理集成了spark sql 、spark dataframe的能力。基于这些能力去编码。对外屏蔽了流切分的过程。RD只需要关注数据源如何处理即可，与正常的spark程序无区别。
2.流处理提供了额外的计算能力。比如说Window操作、Watermark与延迟数据处理等。
3.window的价值
数据源源不断的流入，处理，结果一致保存也不合适。
事实上经过一段时间后，数据收不到很久之前的时间点的数据了，也就不会更新很久时间的时间段的统计值。
因此统计结果应该分时段统计 --> 保存。

二、Window操作
1.背景
上一节，我们知道数据是如何处理的，即可以一个一个处理，也可以一批一批处理。
但数据处理了后一直保留在内存吗？处理多久后的数据可以不用关注了？否则几年的数据一直保留也不现实。
上一节课是对数据进行了批处理，即数据进行了分段处理。另外一种方式是计算进行分段处理，即每一段时间只统计这一段时间的数据。多个时间段的数据可以通过sum等方式进行组合。

window就是规定一段时间，处理整个时间段的数据。
window的结果只会统计本次window范围内的数据。相当于对统计结果进行了分段。


窗口划分:
Tumbling Window 划分出来的时间窗口“不重不漏”。
Sliding Window  划分出来的窗口，可能会重叠、也可能会有遗漏。由窗口间隔Interval与窗口大小Size决定。

2.举例
基于窗口统计每一个window窗口内的单词频率分布。

df = df.withColumn("inputs", split($"value", ","))
.withColumn("eventTime", element_at(col("inputs"),1).cast("timestamp")) //提取事件时间
.withColumn("words", split(element_at(col("inputs"),2), " ")) // 提取单词序列
.withColumn("word", explode($"words")) // 拆分单词
.withWatermark("eventTime", "10 minute") // 启用Watermark机制，指定容忍度T为10分钟
.groupBy(window(col("eventTime"), "5 minute"), col("word")) // 按照Tumbling Window与单词做分组,即每个5分钟统计一次。
.count() // 统计计数

输入内容
2021-10-01 09:30:00,Apache Spark
2021-10-01 09:34:00,Spark Logo
2021-10-01 09:36:00,Structured streaming
2021-10-01 09:39:00,Spark Streaming

输出:
batch1 处于第一个5分钟内的时间段的统计结果
{2021-10-01 09:30:00, 2021-10-01 09:35:00} Apache 1
{2021-10-01 09:30:00, 2021-10-01 09:35:00} Spark  1

batch2 处于第一个5分钟内的时间段的统计结果
{2021-10-01 09:30:00, 2021-10-01 09:35:00} Logo 1
{2021-10-01 09:30:00, 2021-10-01 09:35:00} Spark  2

batch3 
{2021-10-01 09:35:00, 2021-10-01 09:40:00} Structured 1
{2021-10-01 09:35:00, 2021-10-01 09:40:00} streaming  1

batch4
{2021-10-01 09:35:00, 2021-10-01 09:40:00} streaming 2
{2021-10-01 09:35:00, 2021-10-01 09:40:00} Spark  1

3.原理
a.同一个数据，会进入匹配的多个window内参与计算。
b.window的结果只会统计本次window范围内的数据。相当于对统计结果进行了分段。
c.不难推断，随着时间向前推进，已经计算过的窗口，将不会再有状态上的更新。
比如当收到2021-10-01 09:39:00,Spark Streaming 这个消息,理论上“{2021-10-01 09:30:00, 2021-10-01 09:35:00}”这个时间段内的消息一定不会再被接收了，
因此统计结果也不应该有变化。因此应该对窗口window进行存储。

d.说到这里，你可能会有这样的疑问：“那不见得啊！如果在消息39之后，引擎又接收到一条事件时间落在窗口30-35的消息，那该怎么办呢？”
要回答这个问题，我们还得从Late data和Structured Streaming的Watermark机制说起。

三、Late data与Watermark
1.Late data背景
因为数据生产与数据接收处理的时间区间可能不一致，原因是网络抖动等。
比如10:35生产的数据，可能10:45才收到。这样的数据就称之为Late data数据。

即规定一个Late时间，超过该时间还收到的消息,就称之为Late data数据。

2.由于有Late data的存在，流处理引擎就需要一个机制，来判定Late data的有效性，从而决定是否让晚到的消息，参与到之前窗口的计算。
即如果数据已经延迟接收到，那要不要参与到那个时间段的统计计算结果呢？

3.为了解决Late data的问题，Structured Streaming采用了一种叫作Watermark的机制来应对
水印和水位线。

即没定期生产一个水印，设置水印间隔T。 
每次接收到水印时，设置maxTime = 水印 + 水印间隔T。 

如果数据的时间>maxTime,则丢弃该数据。

