
如何理解DAG -- 内存计算的基石
1.定义
DAG全称Direct Acyclic Graph，中文叫有向无环图。
DAG中的每一个顶点都由RDD构成，对应到上图中就是带泥的土豆potatosRDD，清洗过的土豆cleanedPotatosRDD。
DAG的边则标记了不同RDD之间的依赖与转换关系。很明显，上图中DAG的每一条边都有指向性，而且整张图不存在环结构。

2.DAG是怎么生成的呢？
从开发者的视角出发，DAG的构建是通过在分布式数据集上不停地调用算子来完成的。

spark开发,实际上就是灵活运营各算子实现业务逻辑,每一个算子封装了计算逻辑,并且数据源依赖上一个RDD数据源,每一个算子过后产生一个新的RDD。
新的RDD会设置dependencies归属到父RDD上，同时把compute属性赋值到算子封装的计算逻辑上。
因此便有了DAG图。


一、第一层理解
内存存储,利用cache讲计算结果存储到内存，方便重复使用。
因此只有复用性高的RDD才应该被cache,而不是随意乱cache。

同时由于DAG图已经固定生产,因此任意阶段的失败,都不需要从头开始重新计算,而是上游任意一个位置都可以回溯数据。因此找到上游cache的地方去回溯数据即可。

二、第二层理解 内存计算 stage为重点

1.DAG是如何切分成stage的呢?stage的作用是什么？
DAG毕竟只是一张流程图，Spark需要把这张流程图转化成分布式任务，才能充分利用分布式集群并行计算的优势。


2.回溯DAG并划分Stages,分布式集群按照stage的方式分发任务即可。
如果用一句话来概括从DAG到Stages的转化过程，那应该是：以Actions算子为起点，从后向前回溯DAG，以Shuffle操作为边界去划分Stages。
即遇到action算子,则启动一个job。
action算子内,划分多个Stages,划分标准是数据是否分发,即shuffle。 即shuffle的map端为上一个stage的结尾,shuffle的reduce端为下一个stage的开始。

3.stage的理解:
设计师/开发RD在代码上设计,做到每一个stage独立生产一个产品零件,尽量不需要别人-即不需要shuffle分发。
如果遇到需要别人的时候,那就产生shuffle,即拆分成另外一个stage来承接。
相当于每一个stage都是一个大逻辑的java方法体或者接口。


