一、背景与总结
1.为什么有一些场景,自己加了Cache之后，执行性能反而变差了。
原因就是到处都cache,退化为MapReduce，所有数据都从磁盘读取，所以性能就会变慢。

2.Spark的Cache机制主要有3个方面需要我们掌握，它们分别是：
缓存的存储级别：它限定了数据缓存的存储介质，如内存、磁盘等
缓存的计算过程：从RDD展开到分片以Block的形式，存储于内存或磁盘的过程
缓存的销毁过程：缓存数据以主动或是被动的方式，被驱逐出内存或是磁盘的过程

二、缓存的存储级别
1.比如MEMORY_AND_DISK_SER_2、MEMORY_ONLY等等。这些长得差不多的字符串我们该怎么记忆和区分呢？其实，每一种存储级别都包含3个基本要素。
a.存储介质：内存还是磁盘，或是两者都有。
b.存储形式：对象值还是序列化的字节数组，带SER字样的表示以序列化方式存储，不带SER则表示采用对象值。
c.副本数量：存储级别名字最后的数字代表拷贝数量，没有数字默认为1份副本。

2.级别list名单
类型					存储介质			存储形式			副本数量		备注
MEMORY_ONLY			内存				对象值			1
MEMORY_ONLY_2		内存				对象值			2
MEMORY_ONLY_SER		内存				序列化			1
MEMORY_ONLY_SER_2	内存				序列化			2
DISK_ONLY			磁盘				序列化			1
DISK_ONLY_2			磁盘				序列化			2
MEMORY_AND_DISK     内存+磁盘			对象值+序列化		1			内存存储对象值,磁盘存储序列化
MEMORY_AND_DISK_2   内存+磁盘			对象值+序列化		2			内存存储对象值,磁盘存储序列化
MEMORY_AND_DISK_SER 内存+磁盘			序列化			1
MEMORY_AND_DISK_SER_2内存+磁盘		序列化			2

尽管缓存级别多得让人眼花缭乱，但实际上最常用的只有两个：MEMORY_ONLY和MEMORY_AND_DISK，
它们分别是RDD缓存和DataFrame缓存的默认存储级别。
在日常的开发工作中，当你在RDD和DataFrame之上调用.cache函数时，Spark默认采用的就是MEMORY_ONLY和MEMORY_AND_DISK。

三、缓存的计算过程
1.在MEMORY_AND_DISK模式下，Spark会优先尝试把数据集全部缓存到内存，内存不足的情况下，再把剩余的数据落盘到本地。
MEMORY_ONLY则不管内存是否充足，而是一股脑地把数据往内存里塞，即便内存不够,剩余数据丢弃。

2.数据缓存的流程
a.数据分片都是以迭代器Iterator的形式存储的
b.先得把迭代器展开成实实在在的数据值，这一步叫做Unroll，展开的对象值暂时存储在一个叫做ValuesHolder的数据结构里，然后转换为MemoryEntry。
最终，MemoryEntry和与之对应的BlockID，以Key、Value的形式存储到哈希字典（LinkedHashMap）中。

即内存中存储的是hashmap，key是数据块ID,value是数据块内容的值对象MemoryEntry。
序列化到磁盘上,则key依然是数据块ID，value是序列化后的MemoryEntry。

四、缓存的销毁过程
1.我们也应当及时清理用过的Cache，尽早腾出内存空间供其他数据集消费，从而尽量避免Eviction的发生。
异步模式：调用unpersist()或是unpersist(False)
同步模式：调用unpersist(True)
推荐使用异步方式，避免driver卡主。

2.spark驱逐(Eviction)数据。
Eviction驱逐的过程算法使用LRU,将访问率最低的数据块删除掉。
这里，Spark使用了一个巧妙的数据结构：LinkedHashMap，这种数据结构天然地支持LRU算法。
LinkedHashMap = hashMap+双向链表。即hashmap存储BlockId，快速找到BlockId对应的链表。从而循环链表，找到对应的MemoryEntry数据块真实内容。
而链表使用的是双向链表，凡是访问过、插入、更新都会放置在双向链表的尾部。
因此，链表头部保存的刚好都是“最近最少访问”的元素。

3.销毁流程
比如现在要存储rdd为100M的数据，内存不足。
因此从LinkedHashMap中不断找到访问最少得数据，删除掉，一直到有100M空间，停止扫描LinkedHashMap。

注意:扫描过程中,遵循“兔子不吃窝边草”的原则，即同属一个RDD的MemoryEntry不会被选中

4.总结下来，在清除缓存的过程中，Spark遵循两个基本原则：
LRU：按照元素的访问顺序，优先清除那些“最近最少访问”的BlockId、MemoryEntry键值对
兔子不吃窝边草：在清除的过程中，同属一个RDD的MemoryEntry拥有“赦免权”

五、什么时候该用cache
1.遵循以下2条基本原则：
如果RDD/DataFrame/Dataset在应用中的引用次数为1，就坚决不使用Cache
如果引用次数大于1，且运行成本占比超过30%，应当考虑启用Cache。

2.注意事项
运行成本占比:某一个RDD数据集计算消耗总时间/作业全部时间的比值。
分母总时长通过UI就可以获取。
分子可以通过对数据的认知，自行判断，比如这部分逻辑就是很耗时，就cache。


六、cache 到底cache的是什么
1.RDD的cache很直观，就是cache的某一个RDD，因此自己知道哪个RDD最常用，那就cache哪个RDD，肯定是没有太多问题的。
2.sql的cache就容易出问题了。
举例
df有20个字段；

//Cache方式一
val cachedDF = df.cache
//数据分析
cachedDF.filter(col2 > 0).select(col1, col2)
cachedDF.select(col1, col2).filter(col2 > 100)
 
//Cache方式二
df.select(col1, col2).filter(col2 > 0).cache
//数据分析
df.filter(col2 > 0).select(col1, col2)
df.select(col1, col2).filter(col2 > 100)
 
//Cache方式三
val cachedDF = df.select(col1, col2).cache
//数据分析
cachedDF.filter(col2 > 0).select(col1, col2)
cachedDF.select(col1, col2).filter(col2 > 100)

方式1,肯定不对，因为20个字段，只用到了2个字段，因此应该cache的是投影结果。
方式2,乍看上去，两条数据分析语句在逻辑上刚好都能利用缓存的数据内容。
但遗憾的是，这两条分析语句都会跳过缓存数据，分别去磁盘上读取Parquet源文件，然后从头计算投影和过滤的逻辑。
这是为什么呢？究其缘由是，Cache Manager要求两个查询的Analyzed Logical Plan必须完全一致，才能对DataFrame的缓存进行复用。

Analyzed Logical Plan是比较初级的逻辑计划，主要负责AST查询语法树的语义检查，确保查询中引用的表、列等元信息的有效性。
像谓词下推、列剪枝这些比较智能的推理，要等到制定Optimized Logical Plan才会生效。
因此，即使是同一个查询语句，仅仅是调换了select和filter的顺序，在Analyzed Logical Plan阶段也会被判定为不同的逻辑计划。

方式3,才是最有效的cache。
