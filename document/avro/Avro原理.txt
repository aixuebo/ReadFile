一、简介
Avro:跨端RPC中间件.它是一个基于二进制数据传输高性能的中间件。
Avro是一个数据序列化的系统，它可以将数据结构或对象转化成便于存储或传输的格式

二、特点
1.丰富的数据结构类型
2.快速可压缩的二进制数据形式（对数据二进制序列化后可节约数据存储空间和网络传输带宽）
3.存储持久数据的文件容器
4.可实现远程过程调用(RPC)
5.简单的动态语言结合功能
Avro和动态语言结合后，读写数据文件和使用RPC协议都不需要生成代码，而代码生成作为一种可选的优化只需要在静态类型语言中实现。

三、Schema
Avro依赖于模式（Schema），通过模式定义各种数据结构。
同时可动态加载相关数据的模式，数据的读写都使用模式，这使得数据之间不存在任何其他标识（类比Thrift），这样就减少了开销，
使得序列化快速又轻巧，同时这种数据及模式的自我描述也方便了动态脚本语言的使用。

当数据存储到文件中时，它的模式也随之存储，这样任何程序都可以对文件进行处理。
如果读取数据时使用的模式与写入时使用的模式不同，也容易解决，因为读取和写入的模式都是已知的。看下面这个规则：

新模式	Writer	Reader	规则
增加了字段	采用旧的模式（新增前）	采用新的模式	Reader对新字段会使用其默认值（Writer不会为新增的字段赋值）
采用新的模式（新增后）	采用旧的模式	Reader会忽略掉新曾的字段，Writer会为新增字段赋值
减少了字段	采用旧的模式（减少前）	采用新的模式	Reader会忽略已经被删除的字段
采用新的模式（减少后）	采用旧的模式	如果旧模式里被删除的那个字段有默认值，那么Reader会采用，否则，Reader会报错

四、类型
1.简单类型
null，boolean，int，long，float，double，bytes，string。
如：{“type”: “string”}
2.复杂类型---仅仅定义类型
a.record---说明是一个对象,定义对象中每一个属性以及该属性对应的类型、默认值等信息
{
    "type": "record",
    "name": "名称json字符串[必填]",
    "namespace": "命名空间json字符串[选填]",
    "doc": "文档json字符串[选填]"
    "aliases": "别名json字符串数组[选填]",
    "fields": [    //json数组[必填]
        {
            "name": "字段名",
            "type": "类型",
            "default": "默认值",
            "order": "字段顺序"
        },
        ....
    ]
}
例如
{
    "type": "record",
    "name": "test",
    "fields": [
        {"name": "a", "type": "long"},
        {"name": "b", "type": "string"}
    ]
}
b.enum---说明该类型是枚举类型,只能有symbols以下内容之一组成
{
    "type": "enum",
    "name": "名称json字符串[必填]",
    "namespace": "命名空间json字符串[选填]",
    "doc": "文档json字符串[选填]"
    "aliases": "别名json字符串数组[选填]",
    "symbols": [ //json数组[必填]
        "值json字符串[必填]，所有值必须是唯一的"
    ]
}

例如
{
    "type": "enum",
    "name": "test",
    "symbols": ["k", "a", "z", "a", "ff"]
}

c.数组---定义是一个数组类型,定义数组本身是int还是其他类型
{
    "type": "array",
    "items": "子元素模式"
}

例如：
{
    "type": "array",
    "items": "long"
}

d.map类型--规定key一定是String类型的,因此只要设置好value的类型即可构建一个map对象,因为map对象已经规定死key和value类型
{
    "type": "map",
    "values": "值元素模式"
}

例如
{"type" : "map", "values" : "int"}

e.Fixed---被使用存储二进制数据,因此要设置好固定的二进制数组大小,该对象相当于java的byte[]
注意该size是一个整型,单位是1-byte
{
    "type": "fixed",
    "name": "名称json字符串[必填]",
    "namespace": "命名空间json字符串[选填]",
    "aliases": "别名json字符串数组[选填]",
    "size" : "整型，指定每个值的字节数[必填]"
}

例如
{"type" : "fixed" , "name" : "bdata", "size" : 1048576}


f.Unions---用于存储多种数据类型的对象,相当于json的数组模式,即[aa:"",bb:1]
例如:
你可以在type中加入[],用来表示该值可以是String也可以是null。
{
     "type": "record",
     "namespace": "com.example",
     "name": "FullName",
     "fields": [
       { "name": "first", "type": ["string", "null"] },
       { "name": "last", "type": "string", "default" : "Doe" }
     ]
}

五、序列化与反序列化---支持两种模式,二进制模式以及json模式,二进制模式对压缩、传输都是很好的性能
1.简单类型
类型	编码	例子
null	0字节	Null
boolean	1字节	{true: 1, false: 0}
int/long	variable-length zig-zag coding
float	4字节	Java’s floatToIntBits
double	8字节	Java’s floatToIntBits
bytes	一个表示长度的long值，后跟指定长度的字节序列
string	一个表示长度的long值，后跟UTF-8字符集的指定长度的字节序列	“foo”:{3,f,o,o}
2.复杂类型
a.records类型会按字段声明的顺序串连编码值，
例如下面这个record schema：
{
    "type": "record",
    "name": "test",
    "fields": [
        {"name": "a", "type": "long"},
        {"name": "b", "type": "string"}
    ]
}
实例化这个record，假设给a字段赋值27(编码为0x36)，给b字段赋值“foo”(06 66 6f 6f，注意第一个06表示字符串长度3的编码值)，
那么这个record编码结果为： 36 06 66 6f 6f
因此就是一个一个数据被串联起来即可

b.enum被编码为一个int，比如：
{
    "type": "enum",
    "name": "test",
    "symbols": ["A", "B", "C", "D"]
}
这将被编码为一个取值范围为[0，3]的int，0表示A，3表示D。
因此无论枚举的值多么长,他都是一个int,而int是一个可变的字节,因此会很短

c.arrays编码为block序列，每个block包含一个long的count值，紧跟着的是array items，一个block的count为0表示该block是array的结尾。
即先记录size,然后每次顺序写入每一个元素的值

d.maps编码为block序列，每个block包含一个long的count值，紧跟着的是key/value对，一个block的count为0表示该block是map的结尾。
即先记录size,然后每次顺序写入每一个元素的key/value值

e.union编码以一个long值开始，表示后面的数据时union中的哪种数据类型。
即先填入一个序号,表面此时使用的是union的第几个数据类型,比如Strng,就填写1,表示此时读取的是String类型的数据,然后写入具体的数据

f.fixed编码为指定数目的字节
即因为fixed的schema已经知道了固定的长度,因此直接填充长度的字节内容即可

3.如何做到动态的数据格式的
序列化写入数据的时候,虽然把具体的值写入到文件中,但是在文件的最前面要写入schema,这样就可以保证读取文件的时候，知道文件是怎么定义的。
比如定义的schema是学号，姓名，院系和电话。因此存储的时候也是按照这个顺序存储数据的。
在读取的时候,即使要读取的schema只是要姓名和电话,依然通过schema知道具体的数据,将其读取出来。


六、排序
比较规则:
null：总是相等
int,long,float：按照数值大小
boolean：flase在true之前
string：按照字典顺序
bytes，fixed：按照byte的字典顺序
array：按照元素的字典顺序
enum：按照符号在枚举中的位置
record：按照域的字典顺序，如果指定了以下属性：
ascending：域值的顺序不变
descending：域值的顺序颠倒
ignore：排序时忽略域值
map：不可进行排序比较

七、对象容器文件--即如何存储文件---详细参见Avro数据文件图
1.Avro定义了文件格式，一个文件对应一个模式，所有存储在文件中的对象都是根据模式写入的。
但是文件可能会很大,因此文件其实是按照块进行存数，块可以采用压缩的方式存储。
为了在进行MapReduce处理的时候有效的切分文件，在块之间采用了同步记号。

2.一个文件有两部分组成：文件头(Header)和一个或多个文件的数据块(Data Block)。
而头信息由由三部分构成：四个字节的前缀，文件Meta-data信息和随机生成的16字节同步标记符。
目前Avro支持的Meta-data有两种：schema和codec。

codec表示对后面的文件数据块采用何种压缩方式。Avro的实现都需要支持下面两种压缩方式：null（不压缩）和deflate（使用Deflate算法压缩数据块）。
除了文档中认定的两种Meta-data，用户也可以自定义适用于自己的Meta-data，这里用long型来表示有多少个Meta-data数据对，
也是让用户在实际应用中可以定义足够的Meta-data信息。

对于每对Meta-data信息，都有一个string型的key（要以“avro.”为前缀）和二进制编码后的value。
由于对象可以组织成不同的块，使用时就可以不经过反序列化而对某个数据块进行操作。
还可以由数据块数，对象数和同步标记符来定位损坏的块以确保数据完整性。

3.数据块格式:long有多少条数据、压缩后一共多少个字节、序列化后的字节内容、同步符号


八、RPC
1.当在RPC中使用Avro时，服务器和客户端可以在握手连接时交换模式，服务器和客户端有彼此全部的模式，
因此相同命名字段，缺失字段和多余字段等信息之间通信中需要处理的一致性问题就可以解决。

客户端希望同服务器端交互时，就需要交换双方通信的协议，它类似于模式，需要双方来协商定义，在Avro中被称为消息。

2.具体RPC原理 日后使用的时候再详细看