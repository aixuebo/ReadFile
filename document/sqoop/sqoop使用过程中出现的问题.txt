import
--connect
jdbc:mysql://ip:5306/xxx?useCursorFetch=true&dontTrackOpenResources=true&defaultFetchSize=2000&autoReconnect=true&connectTimeout=3000&socketTimeout=60000&tinyInt1isBit=false
--driver
com.mysql.jdbc.Driver
--username
name
--password
pass
--outdir
/server/app/sqoop/vo
--hive-database
dabaname
--hive-import
--hive-overwrite

1.JDBC在mysql读取大表的时候内存溢出
2.使用sqoop将mysql数据库导入到hive中
a.发现passport密码没办法保存在job中,因此修改配置文件conf/sqoop-site.xml
<property>
	<name>sqoop.metastore.client.record.password</name>
	<value>true</value>
	<description>If true, allow saved passwords in the metastore. </description>
</property>
b.没办法在hadoop集群上跑sqoop,因为目录的权限不对
  su hdfs
  hdfs dfs -chown -R root:hdfs /user/root 执行hdfs dfs命令,为该目录赋予权限,user是root 组是hdfs,目录是/user/root -R表示递归该目录下所有文件都是这个权限
c.--target-dir /tmp/test/sqoop/sqoop/regular_info
  表示sqoop输出到什么目录上去
d.create database if not exists xxx location '/tmp/test/sqoop/sqoop';
  先要创建hive的数据库
e.发现user_info中status 的bit类型,hive不支持,因此使用--columns属性,只要需要的列被导出到hive中
f.--hive-overwrite对hive存在的表数据进行覆盖,但是依然发现说临时目录存在,不允许覆盖,原因是没有加入参数--delete-target-dir
g.-m 30 让多个map去抓去程序,这个值不适合过大
h.--hive-database xxx指定sqoop抓去后存储在哪个hive中
i.--hive-import 说明该sqoop输出的内容是要到hive中的
j.--driver com.mysql.jdbc.Driver 有时候报错,需要加入这个
k.写成配置文件形式
sqoop  --options-file /var/sqoop/ljc_import_hive.txt --delete-target-dir -m 10 --table regular_info
其中ljc_import_hive.txt内容
import
--connect
jdbc:mysql://host:port/xxxx?useCursorFetch=true&dontTrackOpenResources=true&defaultFetchSize=2000&useCursorFetch=true&autoReconnect=true&tinyInt1isBit=false
--driver
com.mysql.jdbc.Driver
--username
xxx
--password
xxx
--hive-database
hiveDabaseName
--hive-import
--hive-overwrite
注意
已经没有\&转义了
每一个参数必须占用一行


4.使用sql进行import
a.sqoop --options-file /path/import_hive_test.txt --delete-target-dir -m 23
--target-dir path/aa
--split-by id --query "SELECT * FROM bank_card WHERE id = 88023 and \$CONDITIONS"
b.
sqoop  --options-file /path/import_hive_test.txt --delete-target-dir -m 23 --table bank_card
--where "id = 88023"

sqoop  --options-file /path/import_hive_test.txt --delete-target-dir -m 23 --table bank_card
--columns id,userId,userRealName,userIdNo,bankCard,bankName,isEffective,source,lastUpdateTime

c.导出数据到分区内,sqoop支持一个字段的分区,其实也够了,毕竟按照时间这一个分区就可以搞定导出功能了
sqoop  --options-file /path/import_hive_test.txt --delete-target-dir -m 23 \
--table bank_card --hive-partition-key log_day --hive-partition-value ${d}

5.为什么mysqldump比jdbc快
马明，mysqldump是比JDBC快很多，原因是mysqldump是锁整个库和表后，直接从数据库引擎查出数据。
JDBC需要语法解析，建立连接，然后去通过查询引擎去查找数据
。查找的数据不是全量的，
和数据库的buffer有关，从buffer中读完后才能接着往里写数据。而且还面临其他客户端对数据库锁的争用。

6.修改sqoop的代码,适应业务需求
1.修改sqoop代码,对不规范的数据进行处理
org.apache.sqoop.lib.JdbcWritableBridge
  public static String convertString(String value){
	  return value == null ? null : value.trim();
  }
  public static String readString(int colNum, ResultSet r) throws SQLException {
    return convertString(r.getString(colNum));
  }
测试
select * from bank_card where id = 88023;
主要是发现
a.bank_card表数据有很多应该有值的字段变成null了
b.user_info表有id 和userid 是null的,原因也是回车问题导致的串行

5.支持分区
--hive-partition-key log_create_date --hive-partition-value 2016-11-11 --hive-table new_table_name

6.重要参数
a.当mysql中导入到hdfs的数据,默认每一个字段是使用\t进行拆分的,因此hive会按照\t去分割数据文件成字段,但是有时候数据库描述信息是有\t内容的,因此会导致数据串行
在import的时候加入参数  --fields-terminated-by '\001'可以将mysql的拆分符号设置为acsii的1,这样就可以了

--lines-terminated-by '\021' 设置行分隔符,该值设置以0开头表示的是8进制

用于读取mysql数据的时候,如何拆分一行数据
--input-fields-terminated-by <char>      Sets the input field separator
--input-lines-terminated-by <char>       Sets the input end-of-line

用于如何往hive中写入
--fields-terminated-by
--lines-terminated-by
比如 上面说的,sqoop把数据写入到hdfs上,默认使用\t拆分,但是某一个字段的value值是有\t的,那么就会有错误.因此使用 --fields-terminated-by '\001'为hdfs上的输出设置成新的分隔符,不是用\t,就解决了该问题

7.mysql tinyint类型转换成boolean类型分析
jdbc会把tinyint 认为是Java.sql.Types.BIT，然后sqoop就会转为Boolean了,悲剧吧
解决办法：
a.在连接上加上一句话tinyInt1isBit=false
比如 jdbc:mysql://localhost/test?tinyInt1isBit=false
b.产生这种问题是因为mysql设置的tinyint(1),可以将其设置成tinyint(4)就没问题了
