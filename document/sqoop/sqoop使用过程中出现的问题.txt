import
--connect
jdbc:mysql://ip:5306/xxx?useCursorFetch=true&dontTrackOpenResources=true&defaultFetchSize=2000
--driver
com.mysql.jdbc.Driver
--username
name
--password
pass
--outdir
/server/app/sqoop/vo
--hive-database
dabaname
--hive-import
--hive-overwrite

1.JDBC在mysql读取大表的时候内存溢出
2.使用sqoop将mysql数据库导入到hive中
a.发现passport密码没办法保存在job中,因此修改配置文件conf/sqoop-site.xml
<property>
	<name>sqoop.metastore.client.record.password</name>
	<value>true</value>
	<description>If true, allow saved passwords in the metastore. </description>
</property>
b.没办法在hadoop集群上跑sqoop,因为目录的权限不对
  su hdfs
  hdfs dfs -chown -R root:hdfs /user/root 执行hdfs dfs命令,为该目录赋予权限,user是root 组是hdfs,目录是/user/root -R表示递归该目录下所有文件都是这个权限
c.--target-dir /tmp/test/sqoop/sqoop/regular_info
  表示sqoop输出到什么目录上去
d.create database if not exists xxx location '/tmp/test/sqoop/sqoop';
  先要创建hive的数据库
e.发现user_info中status 的bit类型,hive不支持,因此使用--columns属性,只要需要的列被导出到hive中
f.--hive-overwrite对hive存在的表数据进行覆盖,但是依然发现说临时目录存在,不允许覆盖,原因是没有加入参数--delete-target-dir
g.-m 30 让多个map去抓去程序,这个值不适合过大
h.--hive-database xxx指定sqoop抓去后存储在哪个hive中
i.--hive-import 说明该sqoop输出的内容是要到hive中的
j.--driver com.mysql.jdbc.Driver 有时候报错,需要加入这个
k.写成配置文件形式
sqoop  --options-file /var/sqoop/ljc_import_hive.txt --delete-target-dir -m 10 --table regular_info
其中ljc_import_hive.txt内容
import
--connect
jdbc:mysql://host:port/xxxx?useCursorFetch=true&dontTrackOpenResources=true&defaultFetchSize=2000&useCursorFetch=true
--driver
com.mysql.jdbc.Driver
--username
xxx
--password
xxx
--hive-database
hiveDabaseName
--hive-import
--hive-overwrite
注意
已经没有\&转义了
每一个参数必须占用一行


4.使用sql进行import
a.sqoop --options-file /path/import_hive_test.txt --delete-target-dir -m 23
--target-dir path/aa
--split-by id --query "SELECT * FROM bank_card WHERE id = 88023 and \$CONDITIONS"
b.
sqoop  --options-file /path/import_hive_test.txt --delete-target-dir -m 23 --table bank_card
--where "id = 88023"

sqoop  --options-file /path/import_hive_test.txt --delete-target-dir -m 23 --table bank_card
--columns id,userId,userRealName,userIdNo,bankCard,bankName,isEffective,source,lastUpdateTime

c.导出数据到分区内,sqoop支持一个字段的分区,其实也够了,毕竟按照时间这一个分区就可以搞定导出功能了
sqoop  --options-file /path/import_hive_test.txt --delete-target-dir -m 23 \
--table bank_card --hive-partition-key log_day --hive-partition-value ${d}

5.为什么mysqldump比jdbc快
马明，mysqldump是比JDBC快很多，原因是mysqldump是锁整个库和表后，直接从数据库引擎查出数据。
JDBC需要语法解析，建立连接，然后去通过查询引擎去查找数据
。查找的数据不是全量的，
和数据库的buffer有关，从buffer中读完后才能接着往里写数据。而且还面临其他客户端对数据库锁的争用。

6.修改sqoop的代码,适应业务需求
1.修改sqoop代码,对不规范的数据进行处理
org.apache.sqoop.lib.JdbcWritableBridge
  public static String convertString(String value){
	  return value == null ? null : value.trim();
  }
  public static String readString(int colNum, ResultSet r) throws SQLException {
    return convertString(r.getString(colNum));
  }
测试
select * from bank_card where id = 88023;
主要是发现
a.bank_card表数据有很多应该有值的字段变成null了
b.user_info表有id 和userid 是null的,原因也是回车问题导致的串行
