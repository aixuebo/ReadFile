export
-Dmapreduce.job.queuename=xstorm 设置yarn上的队列
--connect
jdbc:mysql://ip:3306/xxx?useCursorFetch=true&dontTrackOpenResources=true&defaultFetchSize=2000
--username
name
--password
pass
--connection-manager
org.apache.sqoop.manager.MySQLManager
--outdir
/server/app/sqoop/vo

注意事项:
1.update-insert方式export操作
a.
Error during export: Mixed update/insert is not supported against the target database yet
这个异常为什么把driver去掉就好使了
说明设置了driver,没有设置--connection-manager,则返回GenericJdbcManager,
而他是没有实现update-insert方法的

b.sqoop时候
进行export的时候,如果要insert-update操作,则要设置
--connection-manager org.apache.sqoop.manager.MySQLManager 或者取消driver
在import的时候要设置驱动,不用设置--connection-manager,即要使用GenericJdbcManager即可

c.--update-key mobile,userid
 表示按照userid和mobile两个字段进行更新操作,因此这两个字段有要求,必须是唯一索引或者主键才可以执行mysql的duplicate on update语法进行update操作

d.mysql的唯一索引建立
  PRIMARY KEY (`id`),
  UNIQUE KEY `period_create_date_channel` (`period`,`create_date`,`channel`)
注意:唯一索引是不区分大小写的,因此Tengxun和tengxun是认为同一个字符串,因此插入到唯一索引里面会抛异常,执行duplicate on update会进行update操作.
因此会造成数据不准确的问题。
修改`channel` varchar(255) DEFAULT NULL, 改成 `channel` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,即可

e.分别表示当一个null的值出现在string列和非String列的时候,该替换成什么值,用于import和export中
--null-string <null-string>	The string to be written for a null value for string columns
--null-non-string <null-string>	The string to be written for a null value for non-string columns
eg
--null-string '\\N' 在import的时候,遇见数据库是null的,则生成\N到hive中
参考
SqoopOptions的nullNonStringValue
BaseSqoopTool的NULL_NON_STRING = "null-non-string";
ClassWriter中的stringifierForType方法

执行逻辑
import查询数据库,将查询结果ResultSet传入到javaBean中,执行value.readFields(results);方法,从结果集中返回数据,生成对应的value对象,即从rs中获取数据中到value中
经过该方法后,该对象所有的属性都已经有值了,在readField方法中调用this.create_date = JdbcWritableBridge.readString(1, __dbResults);
而JdbcWritableBridge.readString方法逻辑是
  public static Integer readInteger(int colNum, ResultSet r)
      throws SQLException {
    int val;
    val = r.getInt(colNum);
    if (r.wasNull()) {
      return null;
    } else {
      return Integer.valueOf(val);
    }
  }
因此可以看出来最终结果就是有值或者null。

接下来调用toString方法,将结果写入到hdfs上,即最终hive存储的格式就是在toString方法里面,该方法逻辑是
以此在对每一个属性进行输出,输出的内容是colName==null?"this.options.getNullStringValue()":create_date,即如果该属性是null,则执行this.options.getNullStringValue()方法获取内容存储到hdfs上,也就是null的默认值被写入了。


f.因为hive抛出的结果有\N的时候,如果该字段作为主键或者唯一索引插入到数据库的时候,会产生问题,该字段的数据每次插入都会有一行记录,数据库中查询is null便可以查询出来。
如果该列是String的,则--null-string aaa被设置,依然不可用,原因是export的时候不会走--null-string参数。
具体逻辑如下:
hadoop的mr从输入源中读取一行数据,调用vo对象的parse方法,把一行数据作为参数传递进去。
即recordImpl.parse(val);//进行对文本解析
逻辑如下:
  public void parse(Text __record) throws RecordParser.ParseError {
    if (null == this.__parser) {
      this.__parser = new RecordParser(__inputDelimiters);
    }
    List<String> __fields = this.__parser.parseRecord(__record);
    __loadFromFields(__fields);
  }

  private void __loadFromFields(List<String> fields) {
    Iterator<String> __it = fields.listIterator();
    String __cur_str = null;
    try {
    __cur_str = __it.next();
    if (__cur_str.equals("null")) { this.create_date = null; } else {
      this.create_date = __cur_str;
    }

    __cur_str = __it.next();
    if (__cur_str.equals("null")) { this.end_date = null; } else {
      this.end_date = __cur_str;
    }

    __cur_str = __it.next();
    if (__cur_str.equals("null")) { this.channel = null; } else {
      this.channel = __cur_str;
    }
 可以看到最终的值是否是null,如是,则将字段的内容写成null,否则写成具体值,
而什么情况下是null,则走如下逻辑:
  private void parseNullVal(String javaType, String colName, StringBuilder sb) {
    if (javaType.equals("String")) {
      sb.append("    if (__cur_str.equals(\""
         + this.options.getInNullStringValue() + "\")) { this.");
      sb.append(colName);
      sb.append(" = null; } else {\n");
    } else {
      sb.append("    if (__cur_str.equals(\""
         + this.options.getInNullNonStringValue());
      sb.append("\") || __cur_str.length() == 0) { this.");
      sb.append(colName);
      sb.append(" = null; } else {\n");
    }
  }
我们可以看到是getInNullStringValue()与当前的值相同的时候,才会设置为null,因此这部分也使用--null-string参数。

然后调用context.write(recordImpl, NullWritable.get());方法,将结果输出
将结果写到PreparedStatement中即可。
      JdbcWritableBridge.writeString(create_date, 1 + __off, 12, __dbStmt);
	  this.create_date = JdbcWritableBridge.readString(1, __dbResults);

    public static void writeLong(Long val, int paramIdx, int sqlType,
      PreparedStatement s) throws SQLException {
    if (null == val) {
      s.setNull(paramIdx, sqlType);
    } else {
      s.setLong(paramIdx, val);
    }
  }

   public static void writeString(String val, int paramIdx, int sqlType,
      PreparedStatement s) throws SQLException {
    if (null == val) {
      s.setNull(paramIdx, sqlType);
    } else {
      s.setString(paramIdx, val);
    }
  }
  我们发现如果值是null的属性,则直接在jdbc的时候就设置为null了,因此sql中最终结果就是null

2.demo
| mobile | CREATE TABLE `mobile` (
  `mobile` varchar(7) DEFAULT NULL,
  `area` varchar(30) DEFAULT NULL,
  `city` varchar(30) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8 |

sqoop  --options-file /path/export_hive_ykw_dw.txt -m 3
--table mobile --export-dir /path/mobileRegion.txt
--input-fields-terminated-by '\t' --update-key mobile --update-mode allowinsert
--columns mobile,area,city
注意:
--columns的字段顺序与--export-dir文件内的顺序一致即可,--columns中必须包含--update-key中所有的属性,并且最好将--update-key中存在的属性放在--columns最后
--input-fields-terminated-by表示文件--export-dir的分隔符
--update-key mobile 表示按照mobile字段进行查看属于insert还是update操作
--update-mode allowinsert/updateonly allowinsert表示模式使用inser-update模式进行操作,updateonly表示仅仅做更新操作,不做insert操作,遇见insert的就跳过记录即可

3.demo
sqoop --options-file /path/export_hive_ykw_dw.txt -m 1 --table xxx --export-dir /path/log_day=20160531 \
--input-fields-terminated-by '\t' --update-key id --update-mode allowinsert --columns userNum,logDate

4.异常sqoop java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamicis still active
mysql的mysql-connector-java-5.1.17.jar版本太低了,导致以下的两个方法在Date的数据库类型的时候,会抛异常。
          int precision = metadata.getPrecision(i);//属性的整数位
          int scale = metadata.getScale(i);//属性的小数点
解决方法是:更换成mysql-connector-java-5.1.38-bin.jar包即可

5.-m 1 使用一个线程进行export操作,因此可以避免死锁现象产生

6.关于hive上的数据拆分符号,一般hive默认是001,因此--input-fields-terminated-by '\001'即可
官方文档如下:
a character (--fields-terminated-by X)
an escape character (--fields-terminated-by \t). Supported escape characters are:

\b (backspace)
\n (newline)
\r (carriage return)
\t (tab)
\" (double-quote)
\' (single-quote)
\\ (backslash)
\0 (NUL) - This will insert NUL characters between fields or lines, or will disable enclosing/escaping if used for one of the --enclosed-by, --optionally-enclosed-by, or --escaped-by arguments.
The octal representation of a UTF-8 character's code point. This should be of the form \0ooo, where ooo is the octal value. For example, --fields-terminated-by \001 would yield the ^A character.
The hexadecimal representation of a UTF-8 character's code point. This should be of the form \0xhhh, where hhh is the hex value. For example, --fields-terminated-by \0x10 would yield the carriage return character.

7.--export-dir支持通配符,但是该通配符是HDFS的通配符
原因是代码里面是这样使用的Path inputPath = new Path(context.getOptions().getExportDir());//输入数据源目录
因此--export-dir /apps/user_assets_match/* 或者 --export-dir /apps/user_assets_match/{1,2,3}形式


